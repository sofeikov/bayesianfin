[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applications of bayesian modelling in finance",
    "section": "",
    "text": "This project explores a generative approach to pricing natural gas options using a Bayesian GARCH-inspired model. It uses NumPyro to model volatility dynamics from historical data, simulates future prices, and estimates option payoffs under different strike scenarios.\nIt‚Äôs not a production-grade system, but a sandbox to understand how volatility-aware models behave‚Äîand how they might be extended further.\nI decided to go for the following process of modelling the log returns:\n\\[\n\\begin{align*}\n\\log r_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t^2) \\\\\n\\mu_t &= \\alpha_0 + \\sum_{i=1}^S \\alpha_i \\log r_{t-i} \\\\\n\\sigma_t^2 &= \\beta_0 + \\sum_{i=1}^S \\beta_i \\sigma_{t-i}^2\n\\end{align*}\n\\]\n\nüìä Loads and engineers features from natural gas price time series\n‚öôÔ∏è Bayesian model for log returns and autoregressive variance\nüîÅ Generates future price paths via simulation\nüí∞ Computes call option payoffs & model-implied prices\nüîç Fully implemented in NumPyro with MCMC inference\n\n\n\n\n\n\npricepath.png\n\n\n\n\n\n\n\n\ncallpayoffatexpiry.png\n\n\n\n\n\n\n\n\nmodel-implied-price.png",
    "crumbs": [
      "Applications of bayesian modelling in finance"
    ]
  },
  {
    "objectID": "index.html#garch-like-model",
    "href": "index.html#garch-like-model",
    "title": "Applications of bayesian modelling in finance",
    "section": "",
    "text": "This project explores a generative approach to pricing natural gas options using a Bayesian GARCH-inspired model. It uses NumPyro to model volatility dynamics from historical data, simulates future prices, and estimates option payoffs under different strike scenarios.\nIt‚Äôs not a production-grade system, but a sandbox to understand how volatility-aware models behave‚Äîand how they might be extended further.\nI decided to go for the following process of modelling the log returns:\n\\[\n\\begin{align*}\n\\log r_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t^2) \\\\\n\\mu_t &= \\alpha_0 + \\sum_{i=1}^S \\alpha_i \\log r_{t-i} \\\\\n\\sigma_t^2 &= \\beta_0 + \\sum_{i=1}^S \\beta_i \\sigma_{t-i}^2\n\\end{align*}\n\\]\n\nüìä Loads and engineers features from natural gas price time series\n‚öôÔ∏è Bayesian model for log returns and autoregressive variance\nüîÅ Generates future price paths via simulation\nüí∞ Computes call option payoffs & model-implied prices\nüîç Fully implemented in NumPyro with MCMC inference\n\n\n\n\n\n\npricepath.png\n\n\n\n\n\n\n\n\ncallpayoffatexpiry.png\n\n\n\n\n\n\n\n\nmodel-implied-price.png",
    "crumbs": [
      "Applications of bayesian modelling in finance"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "garch_like_modelling.html",
    "href": "garch_like_modelling.html",
    "title": "Modelling option prices with GARCH-like models",
    "section": "",
    "text": "I‚Äôve been interested in financial markets for a while, especially the modeling failures surrounding the 2008 financial crisis. One recurring theme is how standard models struggled to capture risk during volatile periods. That led me to explore financial modeling more seriously, starting with option pricing as a fundamental use case.\nWhile it‚Äôs easy enough to plug numbers into Black-Scholes, I wanted to build something from the ground up‚Äîspecifically a model that learns volatility structure directly from data. So I built a GARCH-inspired, fully Bayesian model using NumPyro that captures both return and variance dynamics.\nThis post walks through that process: loading and transforming market data, fitting the model, simulating future prices, and estimating option payoffs. It‚Äôs a basic framework, but a flexible one, and I‚Äôve also started thinking about ways to extend it‚Äîfor example, conditioning on gas storage data.\nUsing the model‚Äôs generative structure, I simulated forward price paths and estimated call option payoffs at expiration. From these, I computed the likelihood of finishing in the money, and compared the expected payoff to the cost of the option. If the average payoff exceeds the premium, it suggests a positive expected value‚Äîat least under the model‚Äôs assumptions.\nFor now, this assumes European-style options and a buy-and-hold strategy to maturity. The underlying commodity we are targeting isHenry Hub Natural Gas.",
    "crumbs": [
      "Modelling option prices with GARCH-like models"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#data-loading-process",
    "href": "garch_like_modelling.html#data-loading-process",
    "title": "Modelling option prices with GARCH-like models",
    "section": "Data loading process",
    "text": "Data loading process\nWe start by loading the data, and examining the returns, log-return distribution, as well as variance plots.\n\nclass DataLoader:\n    \"\"\"A class for loading and processing time series data with adjustable parameters.\n\n    This class handles loading CSV data, computing returns, and calculating rolling variance.\n\n    Attributes:\n        max_records: Maximum number of records to keep (from the end of the dataset)\n        rolling_variance_window: Window size for calculating rolling variance\n    \"\"\"\n\n    def __init__(self, max_records: int = None, rolling_variance_window: int = 3):\n        self.max_records = max_records\n        self.rolling_variance_window = rolling_variance_window\n\n    def load_data(self, path: str) -&gt; pl.DataFrame:\n        \"\"\"Load and process time series data from a CSV file.\n\n        Args:\n            path: Path to the CSV file containing the data\n\n        Returns:\n            Processed DataFrame with returns and rolling variance\n        \"\"\"\n        df = (\n            pl.read_csv(path, try_parse_dates=True, infer_schema_length=None)\n            .rename({\"Date\": \"date\", \"Price\": \"price\"})\n            .sort(\"date\")\n        )\n        df = df.with_columns(\n            ret=pl.col(\"price\") / pl.col(\"price\").shift(1),\n        )\n\n        if self.max_records is not None:\n            df = df[-self.max_records :]\n\n        return df\n\n\n# Create a data loader with default parameters and load the data\ndata_loader = DataLoader(max_records=9000, rolling_variance_window=3)\nsource_df = data_loader.load_data(\"./data/ng_daily.csv\")\nsource_df.head()\n\n\nshape: (5, 3)\n\n\n\ndate\nprice\nret\n\n\ndate\nf64\nf64\n\n\n\n\n1997-01-07\n3.82\nnull\n\n\n1997-01-08\n3.8\n0.994764\n\n\n1997-01-09\n3.61\n0.95\n\n\n1997-01-10\n3.92\n1.085873\n\n\n1997-01-13\n4.0\n1.020408\n\n\n\n\n\n\nThe class below is implemented to ease off the data analysis and handling. It produces lagged features for returns and variances, as well as transforms the data so it is easier to feed into a model.\n\n\nFeatureEngineer\n\n FeatureEngineer (columns=['ret', 'var'], n_shifts=3,\n                  drop_nulls:bool=True, rolling_variance_window:int=3)\n\n*A class for creating lagged features from time series data.\nThis class handles the creation of lagged (shifted) features that can be used for GARCH-like models and other time series forecasting tasks.\nAttributes: columns: List of column names to create lags for n_shifts: Number of lag periods to create drop_nulls: whether to drop the nulls after rolling window calculations*\n\nfeature_engineer = FeatureEngineer(columns=[\"log_ret\", \"var\"], n_shifts=3)\ndf_with_features = feature_engineer.create_features(source_df)\ndf_with_features.head()\n\n\nshape: (5, 11)\n\n\n\ndate\nprice\nret\nvar\nlog_ret\nprev_log_ret_1\nprev_log_ret_2\nprev_log_ret_3\nprev_var_1\nprev_var_2\nprev_var_3\n\n\ndate\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1997-01-14\n4.01\n1.0025\n0.002433\n0.002497\n0.020203\n0.082384\n-0.051293\n0.042433\n0.024433\n0.013433\n\n\n1997-01-15\n4.34\n1.082294\n0.037433\n0.079083\n0.002497\n0.020203\n0.082384\n0.002433\n0.042433\n0.024433\n\n\n1997-01-16\n4.71\n1.085253\n0.122633\n0.081814\n0.079083\n0.002497\n0.020203\n0.037433\n0.002433\n0.042433\n\n\n1997-01-17\n3.91\n0.830149\n0.1603\n-0.186151\n0.081814\n0.079083\n0.002497\n0.122633\n0.037433\n0.002433\n\n\n1997-01-20\n3.26\n0.83376\n0.5275\n-0.18181\n-0.186151\n0.081814\n0.079083\n0.1603\n0.122633\n0.037433",
    "crumbs": [
      "Modelling option prices with GARCH-like models"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#exploratory-data-analysis",
    "href": "garch_like_modelling.html#exploratory-data-analysis",
    "title": "Modelling option prices with GARCH-like models",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nLet‚Äôs look at various dynamics and distributions associated with this dataset. The first thing we notice is that neither return nor their log counterparts are distributed normally. Instead we see very narrow shapes with random variables that are sufficiently sub-gaussian. From my reading online it is quite normal for commodities, especially natural gas.\n\nsns.histplot(df_with_features, x=\"log_ret\")\nsns.histplot(df_with_features, x=\"ret\")\nplt.xlim([-1, 2])\n\n\n\n\n\n\n\n\nThe 3-day lag variance dynamics are plotted below. As we can see the variance has periods of massive volatility.\n\nplt.figure(figsize=(16, 6))\nsns.lineplot(df_with_features, x=\"date\", y=\"log_ret\")\nplt.figure(figsize=(16, 6))\nsns.lineplot(df_with_features, x=\"date\", y=\"var\")\nplt.ylim(0, 2)",
    "crumbs": [
      "Modelling option prices with GARCH-like models"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#the-model",
    "href": "garch_like_modelling.html#the-model",
    "title": "Modelling option prices with GARCH-like models",
    "section": "The model",
    "text": "The model\nI decided to go for the following process of modelling the log returns:\n\\[\n\\begin{align*}\n\\log r_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t^2) \\\\\n\\mu_t &= \\alpha_0 + \\sum_{i=1}^S \\alpha_i \\log r_{t-i} \\\\\n\\sigma_t^2 &= \\beta_0 + \\sum_{i=1}^S \\beta_i \\sigma_{t-i}^2\n\\end{align*}\n\\]\nWhile this model does not properly model the variance the way a ARCH/GARCH model would, I‚Äôm going to roll with it anyway for the sake of the learning exercise. It is also worth mentioning that there is a conceptual gap between me using the dataset variance for learning and later generating variance myself and essentially continuing in an autoregressive manner. While true, it‚Äôs practically a workable approach and used widely in other fields of ML, e.g.¬†trajectory prediction with generative models. One of the next things I‚Äôm intending to do is to consider volatility to be latent variable and sample it myself, conditioning the site on observed volatility at the time. When it is not observed, it will just be sampled during the generative phase.\n\n# Cut-off point\nT = 300\npresent_value_train, present_value_test = (\n    df_with_features[\"log_ret\"][:-T].to_numpy(),\n    df_with_features[\"log_ret\"][-T:].to_numpy(),\n)\npast_values_train, past_values_test = (\n    feature_engineer.to_numpy_dict(df_with_features[:-T]),\n    feature_engineer.to_numpy_dict(df_with_features[-T:]),\n)\n\n\n\nmodel\n\n model\n        (present_value:numpy.ndarray[typing.Any,numpy.dtype[+_ScalarType_c\n        o]], past_values:dict[str,numpy.ndarray[typing.Any,numpy.dtype[+_S\n        calarType_co]]])\n\nHere‚Äôs a visualization of the model structure to clarify the dependencies.\n\nnumpyro.render_model(\n    model,\n    model_args=(\n        present_value_train,\n        past_values_train,\n    ),\n)\n\n\n\n\n\n\n\n\nThe model if fitted using MCMC with the NUTS sampler.\n\nrng_key = random.PRNGKey(0)\nrng_key, rng_key_ = random.split(rng_key)\n\n# Run NUTS.\nkernel = NUTS(model)\nnum_samples = 2000\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=num_samples)\n\nmcmc.run(\n    rng_key_,\n    present_value=present_value_train,\n    past_values=past_values_train,\n)\nmcmc.print_summary()\n\nposterior_samples = mcmc.get_samples()\nidata = az.from_numpyro(mcmc)\n\nsample: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:06&lt;00:00, 448.94it/s, 31 steps of size 1.47e-01. acc. prob=0.94]\n\n\n\n                            mean       std    median      5.0%     95.0%     n_eff     r_hat\n                     b      0.00      0.00      0.00     -0.00      0.00   1865.07      1.00\n                 b_var      0.00      0.00      0.00      0.00      0.00   1569.63      1.00\n  param_prev_log_ret_1      0.03      0.01      0.03      0.01      0.06   1658.50      1.00\n  param_prev_log_ret_2     -0.10      0.01     -0.10     -0.12     -0.07   1957.84      1.00\n  param_prev_log_ret_3     -0.03      0.01     -0.03     -0.06     -0.01   1554.26      1.00\n      param_prev_var_1      0.04      0.00      0.04      0.03      0.04   1454.73      1.00\n      param_prev_var_2      0.00      0.00      0.00      0.00      0.00   1970.33      1.00\n      param_prev_var_3      0.02      0.00      0.02      0.02      0.02   1565.87      1.00\n\nNumber of divergences: 0\n\n\nThe model seems to be converging well(notice the \\(\\hat{r}\\)), let us now explore basic diagnostics for the model. Starting with the forest plot of the posterior. I can immediately notice two interesting things: both biases for log return and variance are 0. Also, all variance terms except for the previous one are essentially 0 too, meaning that only the variance of the previous 3 days is important. We might very well have selected a bad time frame and we need to experiment with shorter/longer variance aggregation time frames. There is a need to study volatility clustering patterns somehow.\n\naz.plot_forest(posterior_samples)\n\narray([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nLet‚Äôs plot the HMC chains too, which all look adequate and as if sampling happens with high efficiency. The only odd chain is for the second lag of the variance, but that coefficient is estimated to be 0, which is consistent with higher order lags, so it seems like it can be safely ignored.\n\naz.plot_trace(idata);\n\n\n\n\n\n\n\n\nLet‚Äôs now implement the autoregressive part of the model. Starting from the last known price and update the model. All needed components, such as the predicted log returns that will be used as past values, as well as the variance that will be used autoregressively as well.\n\ndef append_from_log_ret(df: pl.DataFrame, new_log_ret: float) -&gt; pl.DataFrame:\n    \"\"\"Adds a new record to the dataframe based on a log return value.\n\n    Args:\n        df: Input DataFrame containing time series data\n        new_log_ret: The new log return value to add\n\n    Returns:\n        DataFrame with a new row appended\n    \"\"\"\n    # Get the latest date and add one day\n    last_date = df[\"date\"].max()\n    new_date = last_date + timedelta(days=1)\n\n    # Calculate the new return value from log return\n    new_ret = np.exp(new_log_ret)\n\n    # Get the last price and calculate the new price\n    last_price = df[\"price\"].tail(1).item()\n    new_price = last_price * new_ret\n\n    # Create a new row\n    new_row = pl.DataFrame(\n        {\n            \"date\": [new_date],\n            \"price\": [new_price],\n            \"ret\": [new_ret],\n        }\n    )\n\n    # Append the new row to the existing DataFrame\n    return pl.concat([df, new_row])\n\nFinally ready to generate some paths for the natural gas prices. This is a simple autoregressive loop wherein the newly generated data is fed as input into the next step.\n\nnum_sims = 1000\nall_trajectories = []\n\n# We want a single sample when going through the auto-regressive part\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\ntarget_site = \"log_ret\"\n\n# Generate some randomness for sampling\nrng_key = random.PRNGKey(1)\nrng_key, sim_key = random.split(rng_key)\nfor sim_id in tqdm(range(num_sims)):\n    sim_key, traj_key = random.split(sim_key)\n\n    # Create the predictive object to actually sample log returns\n    prior_predictive = Predictive(\n        model,\n        posterior_samples=posterior_for_gen,\n        num_samples=1,\n    )\n\n    # Init the starting price and the features\n    starting_sim_df = source_df[\n        -T\n        - feature_engineer.n_shifts * feature_engineer.rolling_variance_window\n        - 50 : -T\n    ]\n    feature_sim_df = feature_engineer.create_features(starting_sim_df)\n    current_price_shifts = feature_engineer.to_numpy_dict(feature_sim_df[-1])\n    for t in range(min(T, 30)):\n        # Obtain prior predictions\n        traj_key, step_key = random.split(traj_key)\n        prior_predictions = prior_predictive(\n            step_key,\n            present_value=None,\n            past_values=current_price_shifts,\n        )\n        # Takes any samples from the site and record it\n        new_log_ret = prior_predictions[target_site].squeeze().item()\n        starting_sim_df = append_from_log_ret(starting_sim_df, new_log_ret=new_log_ret)\n\n        # With the new record attached, we re-extract the features.\n        feature_sim_df = feature_engineer.create_features(starting_sim_df)\n        current_price_shifts = feature_engineer.to_numpy_dict(feature_sim_df[-1])\n    all_trajectories.append(starting_sim_df.with_columns(run_id=pl.lit(sim_id)))\nall_runs = pl.concat(all_trajectories)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:48&lt;00:00, 20.62it/s]\n\n\n\nsns.lineplot(all_runs, x=\"date\", y=\"price\", hue=\"run_id\")\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\nAs we can see on the charts below, the generated paths exhibit generally adequate behavior. Very few paths deviate and end up too high, and there is no degenerate behaviour with negative prices or prices near 0. While negative prices did happen in 2022 for oil, that is not typical and not something one could reasonably predict.\nI now turn my attention to looking at the distribution for the final prices among the generated paths. We can see a somewhat healthy asymmetric distribution with a few high price examples on the right hand side. We can clearly see that the paths generated show there is some reasonable expected price behavior.\n\nlast_prices = (\n    all_runs.sort(\"date\")  # Ensure dates are in correct order\n    .group_by(\"run_id\")\n    .agg(pl.col(\"price\").last())  # Get the last price for each run\n    .rename({\"price\": \"final_price\"})\n)\nsns.histplot(last_prices, x=\"final_price\")\n\n\n\n\n\n\n\n\nNext, we calculate the call option payoff distributions, as well as model-implied option prices.\n\nstrike_prices = [2.5, 3.0, 3.5, 4.0]\n\n# Create a list of DataFrames, one per strike\npayoff_dfs = [\n    last_prices.with_columns(\n        payoff_at_expiry=(pl.col(\"final_price\") - strike).clip(0),\n        strike=pl.lit(strike),  # so we can track which row belongs to which strike\n    )\n    for strike in strike_prices\n]\n\n# Concatenate into one big DataFrame\npayoff_df_all = pl.concat(payoff_dfs)\nplt.figure(figsize=(12, 6))\nsns.histplot(\n    data=payoff_df_all,\n    x=\"payoff_at_expiry\",\n    hue=\"strike\",\n    element=\"step\",\n    stat=\"count\",\n    bins=40,\n)\nplt.title(\"Call Option Payoff Distribution at Expiry\")\nplt.xlabel(\"Payoff at Expiry\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group and average payoff per strike\nimplied_prices = payoff_df_all.group_by(\"strike\").agg(pl.col(\"payoff_at_expiry\").mean())\n\n# Plot the pricing curve\nplt.figure(figsize=(10, 5))\nsns.lineplot(data=implied_prices, x=\"strike\", y=\"payoff_at_expiry\", marker=\"o\")\nplt.title(\"Model-Implied Call Option Prices vs Strike\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Fair Price (Expected Payoff)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis wraps up my initial attempt at a generative, volatility-aware option pricing framework. There‚Äôs more work to be done‚Äîespecially around incorporating latent volatility, or testing market-implied vol surfaces‚Äîbut even this simplified model gives useful directional insight. Overall, this project was a small but useful step toward building generative models that respect volatility dynamics, rather than assuming them away. There‚Äôs a long road between ‚Äúreasonable simulations‚Äù and ‚Äúproduction-grade financial modeling,‚Äù but this framework makes experimentation and extension straightforward‚Äîand that‚Äôs exactly the point.",
    "crumbs": [
      "Modelling option prices with GARCH-like models"
    ]
  }
]