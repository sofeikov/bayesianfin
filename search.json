[
  {
    "objectID": "simulator.html",
    "href": "simulator.html",
    "title": "bayesianfin",
    "section": "",
    "text": "source\n\nSimulator\n\n Simulator (model:Callable,\n            feature_engineer:bayesianfin.data.FeatureEngineer,\n            target_site:str='log_ret', inherit_vals:list[str]=(),\n            exo_fixed_effects:list[str]=(),\n            additional_effects:list[str]=())",
    "crumbs": [
      "simulator.html"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html",
    "href": "garch_like_sample_vol.html",
    "title": "GARCH-inspire latent volatility model",
    "section": "",
    "text": "Note\n\n\n\nGenerative AI was used to fix grammar in the writing. The rest is done by me.\nIn the previous post I built a very simple model to demonstrate how log return dynamics can be learnt from previous noisy observations of observed returns. The real problem of that approach was that variance was assumed to be something observed, whereas it is clear that variance is not something we measure directly.\nWe do not measure log returns either, but those were sampled, so we assumed we do not know them. In this post I’m actually going to switch to modelling latent volatility, which is supposedly a better way to build this sort of model.\nRunning a bit ahead of myself, I do think that this sort of modelling leads to better and more reasonable models, but quantifying these statements is something we will deal with later on.\nThis articles describes in details the following flow:\nflowchart TD\n    A[Historical Prices & Features] --&gt; B[Feature Engineering]\n    B --&gt; C[Past Returns and Variances]\n    C --&gt; D[Latent Volatility Sampling]\n    D --&gt; E[Log Return Sampling]\n    E --&gt; F[Reconstruct Price Path]\n    F --&gt; H[Output Price Path Simulation]",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#data-loading",
    "href": "garch_like_sample_vol.html#data-loading",
    "title": "GARCH-inspire latent volatility model",
    "section": "Data loading",
    "text": "Data loading\nWe start by loading the data, and examining the returns, log-return distribution, as well as variance plots. The dataset we are using today contains heating oil prices from 2010 until May, 2025. The dataset was pulled from investing.com.\n\nsource_df = (\n    pl.read_csv(\n        \"./data/heating_oil.csv\",\n        infer_schema_length=0,\n        dtypes={\"Price\": pl.Utf8},\n        columns=[\"Date\", \"Price\"],\n    )\n    .with_columns(\n        pl.col(\"Date\").str.to_date(format=\"%m/%d/%Y\"),\n        pl.col(\"Price\").str.replace_all(\",\", \"\").cast(pl.Float64),\n    )\n    .rename({\"Date\": \"date\", \"Price\": \"price\"})\n    .with_columns(\n        ret=pl.col(\"price\") / pl.col(\"price\").shift(1),\n    )\n).sort(\"date\")\nsource_df.head()\n\n/var/folders/8s/q13s1_m56g3b3k_1fmdrwn_80000gn/T/ipykernel_20067/1155920622.py:2: DeprecationWarning: The argument `dtypes` for `read_csv` is deprecated. It has been renamed to `schema_overrides`.\n  pl.read_csv(\n\n\n\nshape: (5, 3)\n\n\n\ndate\nprice\nret\n\n\ndate\nf64\nf64\n\n\n\n\n2010-04-05\n2.2825\n0.998775\n\n\n2010-04-06\n2.2853\n1.010703\n\n\n2010-04-07\n2.2611\n1.007396\n\n\n2010-04-08\n2.2445\n1.000401\n\n\n2010-04-09\n2.2436\n1.002771\n\n\n\n\n\n\n\nsource_df = DataLoader().load_data(\"data/ng_daily.csv\")\nsource_df\n\n\nshape: (7_060, 3)\n\n\n\ndate\nprice\nret\n\n\ndate\nf64\nf64\n\n\n\n\n1997-01-07\n3.82\nnull\n\n\n1997-01-08\n3.8\n0.994764\n\n\n1997-01-09\n3.61\n0.95\n\n\n1997-01-10\n3.92\n1.085873\n\n\n1997-01-13\n4.0\n1.020408\n\n\n…\n…\n…\n\n\n2025-02-04\n3.25\n0.984848\n\n\n2025-02-05\n3.22\n0.990769\n\n\n2025-02-06\n3.31\n1.02795\n\n\n2025-02-07\n3.32\n1.003021\n\n\n2025-02-10\n3.48\n1.048193",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#feature-extraction",
    "href": "garch_like_sample_vol.html#feature-extraction",
    "title": "GARCH-inspire latent volatility model",
    "section": "Feature extraction",
    "text": "Feature extraction\nThe class below is implemented to ease off the data analysis and handling. It produces lagged features for returns and variances, as well as transforms the data so it is easier to feed into a model.\n\nfeature_engineer = FeatureEngineer(\n    transforms=[\n        LogReturn(source_field=\"ret\", feature_name=\"log_ret\"),\n        Variance(\n            source_field=\"price\",\n            feature_name=\"var\",\n            rolling_variance_window=2,\n        ),\n        LogReturn(source_field=\"var\", feature_name=\"log_var\", requested_lag=0),\n        QuantileTransformer(\n            source_field=\"var\", feature_name=\"var_quantile\", requested_lag=1\n        ),\n    ],\n    n_shifts=3,\n)\ndf_with_features = feature_engineer.create_features(source_df)\ndf_with_features.head()\n\n\nshape: (5, 14)\n\n\n\ndate\nprice\nret\nlog_ret\nvar\nlog_var\nvar_quantile\nprev_log_ret_1\nprev_log_ret_2\nprev_log_ret_3\nprev_var_1\nprev_var_2\nprev_var_3\nprev_var_quantile_1\n\n\ndate\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1997-01-13\n4.0\n1.020408\n0.020203\n0.0032\n-5.744604\n0.511527\n0.082384\n-0.051293\n-0.005249\n0.04805\n0.01805\n0.0002\n0.905906\n\n\n1997-01-14\n4.01\n1.0025\n0.002497\n0.0001\n-9.21034\n0.0\n0.020203\n0.082384\n-0.051293\n0.0032\n0.04805\n0.01805\n0.511527\n\n\n1997-01-15\n4.34\n1.082294\n0.079083\n0.05445\n-2.910472\n0.915916\n0.002497\n0.020203\n0.082384\n0.0001\n0.0032\n0.04805\n0.0\n\n\n1997-01-16\n4.71\n1.085253\n0.081814\n0.06845\n-2.681652\n0.929968\n0.079083\n0.002497\n0.020203\n0.05445\n0.0001\n0.0032\n0.915916\n\n\n1997-01-17\n3.91\n0.830149\n-0.186151\n0.32\n-1.139434\n0.984985\n0.081814\n0.079083\n0.002497\n0.06845\n0.05445\n0.0001\n0.929968\n\n\n\n\n\n\n\nsns.histplot(df_with_features, x=\"log_ret\")\nsns.histplot(df_with_features, x=\"ret\")\nplt.xlim([-1, 2])\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(16, 6))\nsns.lineplot(df_with_features, x=\"date\", y=\"price\")\nplt.figure(figsize=(16, 6))\nsns.lineplot(df_with_features, x=\"date\", y=\"log_ret\")",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#the-model",
    "href": "garch_like_sample_vol.html#the-model",
    "title": "GARCH-inspire latent volatility model",
    "section": "The model",
    "text": "The model\nI will now demonstrate how one can build a latent volatility model that can then be used to sample new observations in an autoregressive manner. The main change here is that the volatility is not a linear function anymore, like it used to be, but it is a latent variable sampled along with the log return. Still, a limitation remains for this model that we do condition our latent volatility on the observed noisy variable. Worth noting, that we assume a latent volatility variable per time step, meaning we will end up with a latent volatility variable that has the length of the original time series.\nAs before, the log return is sampled from Student T distribution so account for potentially heavier tails. Other than that, we still use Monte Carlo to build an idea for posterior distributions. Despite a relatively long time series, the below model sampling is quite efficient, so should converge fast.\n\n# Cut-off point\nT = 300\npresent_value_log_ret, present_value_log_var, present_value_test = (\n    df_with_features[\"log_ret\"][:-T].to_numpy(),\n    df_with_features[\"log_var\"][:-T].to_numpy(),\n    df_with_features[\"log_ret\"][-T:].to_numpy(),\n)\npast_values_train, past_values_test = (\n    feature_engineer.to_numpy_dict(df_with_features[:-T]),\n    feature_engineer.to_numpy_dict(df_with_features[-T:]),\n)\n\n\n\ngarch_like_sample_vol_model\n\n garch_like_sample_vol_model (value_log_ret:numpy.ndarray[typing.Any,numpy\n                              .dtype[+_ScalarType_co]]=None, log_var_value\n                              :numpy.ndarray[typing.Any,numpy.dtype[+_Scal\n                              arType_co]]=None, past_values:dict[str,numpy\n                              .ndarray[typing.Any,numpy.dtype[+_ScalarType\n                              _co]]]=None)\n\n\nnumpyro.render_model(\n    garch_like_sample_vol_model,\n    model_args=(\n        present_value_log_ret,\n        present_value_log_var,\n        past_values_train,\n    ),\n)\n\n\n\n\n\n\n\n\n\nrng_key = random.PRNGKey(0)\nrng_key, rng_key_ = random.split(rng_key)\n\n# Run NUTS.\nkernel = NUTS(garch_like_sample_vol_model)\nnum_samples = 2000\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=num_samples)\n\nmcmc.run(\n    rng_key_,\n    present_value_log_ret,\n    present_value_log_var,\n    past_values_train,\n)\nmcmc.print_summary()\nposterior_samples = mcmc.get_samples()\nprint(posterior_samples.keys())\n\nsample: 100%|██████████| 3000/3000 [00:01&lt;00:00, 2140.04it/s, 11 steps of size 2.40e-01. acc. prob=0.92]\n\n\n\n                                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n                      b_var     -7.02      0.00     -7.02     -7.02     -7.02    663.62      1.00\n  param_prev_var_quantile_1      2.09      0.00      2.09      2.08      2.09    625.89      1.00\n\nNumber of divergences: 0\ndict_keys(['b_var', 'log_ret_var', 'param_prev_var_quantile_1'])",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#post-fitting-checks",
    "href": "garch_like_sample_vol.html#post-fitting-checks",
    "title": "GARCH-inspire latent volatility model",
    "section": "Post-fitting checks",
    "text": "Post-fitting checks\n\nposterior_samples = mcmc.get_samples()\npredictive = Predictive(\n    garch_like_sample_vol_model,\n    posterior_samples=posterior_samples,\n    return_sites=[\"log_ret\"],  # or whatever your observation site is called\n)\nrng_key, rng_key_ppc = random.split(rng_key)\nppc_samples = predictive(\n    rng_key_ppc,\n    present_value_log_ret,\n    present_value_log_var,\n    past_values_train,\n)\nprior_samples = Predictive(garch_like_sample_vol_model, num_samples=2000)(\n    rng_key,\n    present_value_log_ret,\n    present_value_log_var,\n    past_values_train,\n)\n\nidata = az.from_numpyro(mcmc, posterior_predictive=ppc_samples, prior=prior_samples)\n\n\naz.plot_trace(idata, var_names=[\"~^log_ret_var\"], filter_vars=\"regex\");",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#future-paths-simulations",
    "href": "garch_like_sample_vol.html#future-paths-simulations",
    "title": "GARCH-inspire latent volatility model",
    "section": "Future paths simulations",
    "text": "Future paths simulations\nFinally, we are able to run the autoregressive simulation and look at the distribution of the final prices. Let’s look at the average sample error for each of the run id. We then can compare the average spread relative to the\n\nsimulator = Simulator(\n    model=garch_like_sample_vol_model,\n    feature_engineer=feature_engineer,\n)\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\ndel posterior_for_gen[\"log_ret_var\"]\nstarting_sim_df = source_df[-T - feature_engineer.n_shifts * 5 - 50 : -T]\nall_runs = simulator.simulate_paths(\n    steps=30,\n    starting_sim_df=starting_sim_df,\n    posterior_samples=posterior_for_gen,\n    num_sims=200,\n)\nsns.lineplot(all_runs, x=\"date\", y=\"price\", hue=\"run_id\")\nplt.xticks(rotation=90);",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#model-validation-predictive-accuracy",
    "href": "garch_like_sample_vol.html#model-validation-predictive-accuracy",
    "title": "GARCH-inspire latent volatility model",
    "section": "Model validation & predictive accuracy",
    "text": "Model validation & predictive accuracy\nI now turn my attention to testing the predictive power of the model. Posterior checks are good and fun, but the model is autoregressive in nature, so what needs to happen is we need to roll through out test set, simulate a distribution of forward looking prices, then do boxplots, with the predicted price distributions and the actual price inside those boxes. The purpose of this exercise is to see if the actual price tends to be within the range of prices that comes out of simulations.\nIn a no volatility perfectly almost perfectly predictable world, the boxes would be very tight around the actual observations, but realistically there is going to be a lot of wiggle room inside.\n\ntarget_window = 7\nsimulator = Simulator(\n    model=garch_like_sample_vol_model, feature_engineer=feature_engineer\n)\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\nshifts = list(range(0, T, 10))\nall_lasts = []\nactual_prices = []\nfor ct, t in tqdm(enumerate(shifts), total=len(shifts)):\n    # print(f\"Simulating shift {ct}/{len(shifts)}\")\n    starting_sim_df = source_df[-T - feature_engineer.n_shifts * 5 - 50 + t : -T + t]\n    this_sim = simulator.simulate_paths(\n        steps=target_window,\n        starting_sim_df=starting_sim_df,\n        posterior_samples=posterior_for_gen,\n        num_sims=50,\n    )\n    last_prices = (\n        this_sim.sort(\"date\")  # Ensure dates are in correct order\n        .group_by(\"run_id\")\n        .agg(pl.col(\"price\").last())  # Get the last price for each run\n        .rename({\"price\": \"final_price\"})\n        .with_columns(pl.lit(ct).alias(\"run_id\"))\n    )\n    actual_prices.append({\"run_id\": ct, \"actual_price\": source_df[-T + t][\"price\"][0]})\n    all_lasts.append(last_prices)\nall_lasts = pl.concat(all_lasts)\nactual_prices = pl.DataFrame(actual_prices)\n\n100%|██████████| 30/30 [00:25&lt;00:00,  1.19it/s]\n\n\n\nsns.boxplot(data=all_lasts, x=\"run_id\", y=\"final_price\")\nsns.scatterplot(data=actual_prices, x=\"run_id\", y=\"actual_price\", zorder=10)\nplt.ylim([0, 6])\n\n\n\n\n\n\n\n\nWe can finally see that the mean error on the observed values is 25% lower than in the previous version.\n\nall_lasts.join(\n    actual_prices, left_on=\"run_id\", right_on=\"run_id\", how=\"left\"\n).with_columns(error=(pl.col(\"final_price\") - pl.col(\"actual_price\")) ** 2).group_by(\n    \"run_id\"\n).agg(pl.col(\"error\").mean())[\"error\"].median()\n\n0.08732833325394218",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#option-pricing",
    "href": "garch_like_sample_vol.html#option-pricing",
    "title": "GARCH-inspire latent volatility model",
    "section": "Option pricing",
    "text": "Option pricing\nNext, we calculate the call option payoff distributions, as well as model-implied option prices.\n\nlast_prices = (\n    all_runs.sort(\"date\")  # Ensure dates are in correct order\n    .group_by(\"run_id\")\n    .agg(pl.col(\"price\").last())  # Get the last price for each run\n    .rename({\"price\": \"final_price\"})\n)\nsns.histplot(last_prices, x=\"final_price\")\n\n\n\n\n\n\n\n\n\nstrike_prices = [2.5, 3.0, 3.5, 4.0]\n\n# Create a list of DataFrames, one per strike\npayoff_dfs = [\n    last_prices.with_columns(\n        payoff_at_expiry=(pl.col(\"final_price\") - strike).clip(0),\n        strike=pl.lit(strike),  # so we can track which row belongs to which strike\n    )\n    for strike in strike_prices\n]\n\n# Concatenate into one big DataFrame\npayoff_df_all = pl.concat(payoff_dfs)\nplt.figure(figsize=(12, 6))\nsns.histplot(\n    data=payoff_df_all,\n    x=\"payoff_at_expiry\",\n    hue=\"strike\",\n    element=\"step\",\n    stat=\"count\",\n    bins=40,\n)\nplt.title(\"Call Option Payoff Distribution at Expiry\")\nplt.xlabel(\"Payoff at Expiry\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group and average payoff per strike\nimplied_prices = payoff_df_all.group_by(\"strike\").agg(pl.col(\"payoff_at_expiry\").mean())\n\n# Plot the pricing curve\nplt.figure(figsize=(10, 5))\nsns.lineplot(data=implied_prices, x=\"strike\", y=\"payoff_at_expiry\", marker=\"o\")\nplt.title(\"Model-Implied Call Option Prices vs Strike\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Fair Price (Expected Payoff)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn this post I have implemented a latent volatility model, which has a measurable improvement in predictive power for future price scenarios. In the next post I will demonstrate how energy commodities storage levels can be incorporated into this model.",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#limitations-next-steps",
    "href": "garch_like_sample_vol.html#limitations-next-steps",
    "title": "GARCH-inspire latent volatility model",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\nThe model assumes latent volatility follows a stationary process, which may not hold in extreme market events.\nDoes not yet incorporate external factors (e.g., inventory levels, macro data).\nModel not benchmarked against market-implied volatility surfaces.\nFuture work: extend to multivariate commodities or test on real-world options datasets.\nThis is a research-grade toy model; do not use for actual trading decisions.",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "garch_like_sample_vol.html#further-reading",
    "href": "garch_like_sample_vol.html#further-reading",
    "title": "GARCH-inspire latent volatility model",
    "section": "Further reading",
    "text": "Further reading\n\nPrevious article: A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing\nOriginal GARCH paper\nNumPyro docs",
    "crumbs": [
      "GARCH-inspire latent volatility model"
    ]
  },
  {
    "objectID": "volatility_and_storage.html",
    "href": "volatility_and_storage.html",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "",
    "text": "Note\n\n\n\nGenerative AI was used to fix grammar in the writing. The rest is done by me.\nIn the previous post I built a latent volatility model that was used to predict Henry Hub gas prices. While it was a very simple model, it did output meaningful results in the end. One of the fundamental problems with that kind of model is that it makes no assumption on the nature of the underlying time series. It could just as easily be modeling sheep populations in New Zealand.. Today I’m making the first step in addressing that by including a fundamental macro indicator - reported levels of gas storage, collected by US Energy Information Administration. I’d recommend anyone to spend like 30 mins on their website - the amount of historical information on energy is amazing and it’s a great resource all around.\nThe reason I decided to start with storage information is because it is a very strong indicator of the volatility. Depending on weather forecasts, the current levels of storage and other events, volatility goes up and down. For example, in warmer months, NG volatility tends to be lower(unless snow hits Texas), and in colder months when sudden frosts hit, the volatility will increase, as there is more demand for an already depleted commodity.",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#eia-gas-report",
    "href": "volatility_and_storage.html#eia-gas-report",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "EIA gas report",
    "text": "EIA gas report\nOn a weekly basis, EIA collects information about gas storage levels. I will use storage level information from the lower 48 states. And while, there are other massive storages on other continents, US, on its own, being a massive energy market provides a valuable insight into future volatility values.",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#how-to-include-storage-levels",
    "href": "volatility_and_storage.html#how-to-include-storage-levels",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "How to include storage levels",
    "text": "How to include storage levels\nWithin the bayesian framework, there are a few ways to include the storage information. First of all, the EIA methodology document states that the parties reporting their storage levels are sampled, meaning that from week to week we do not get reports from the same parties. Plus, while it is unclear how much evidence there is, it would reasonable to assume that there inaccuracies in those reports. With that in mind, my initial idea was to model the storage levels as a variable with noisy observation, and consider it a latent variable that was periodically observed. That would lead to a two-level bayesian model. However, our NG prices dataset is on the daily basis and the storage reports are on the weekly basis, making it unclear how exactly those two should be aligned, in case the storage information is assumed a latent variable.\nI then decided to use it as an external regressor and leave the more complicated version for future trials. Additionally, I include storage level derivative into the modelling since I found that it helps to make the model more stable, plus conceptually it also makes sense that information about the speed of storage change and its direction are important.\n\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#data-loading",
    "href": "volatility_and_storage.html#data-loading",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Data loading",
    "text": "Data loading\nStart with loading the daily level gas data\n\ngas_data = (\n    pl.read_csv(\"data/ng_daily.csv\", try_parse_dates=True)\n    .drop_nulls()\n    .tail(3500)\n    .rename({\"Date\": \"date\", \"Price\": \"price\"})\n).with_columns(\n    ret=pl.col(\"price\") / pl.col(\"price\").shift(1),\n)\ngas_data\n\n\nshape: (3_500, 3)\n\n\n\ndate\nprice\nret\n\n\ndate\nf64\nf64\n\n\n\n\n2011-04-01\n4.32\nnull\n\n\n2011-04-04\n4.21\n0.974537\n\n\n2011-04-05\n4.22\n1.002375\n\n\n2011-04-06\n4.17\n0.988152\n\n\n2011-04-07\n4.12\n0.98801\n\n\n…\n…\n…\n\n\n2025-02-04\n3.25\n0.984848\n\n\n2025-02-05\n3.22\n0.990769\n\n\n2025-02-06\n3.31\n1.02795\n\n\n2025-02-07\n3.32\n1.003021\n\n\n2025-02-10\n3.48\n1.048193\n\n\n\n\n\n\nI then load storage data, focusing only on the lower 48 states. The relevant column is renamed to storage for convenience.\n\nng_storage_data = (\n    pl.read_csv(\"data/ng_storage.csv\", truncate_ragged_lines=True)\n    .with_columns(\n        pl.col(\"Week ending\").str.strptime(pl.Datetime, \"%d-%b-%y\").alias(\"date\"),\n    )\n    .with_columns(pl.col(\"date\").cast(pl.Date))\n    .rename({\"Total Lower 48\": \"storage\"})\n)\nng_storage_data.head()\n\n\nshape: (5, 10)\n\n\n\nWeek ending\nSource\nEast Region\nMidwest Region Mountain Region\nPacific Region\nSouth Central Region\nSalt\nNonSalt\nstorage\ndate\n\n\nstr\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ndate\n\n\n\n\n\"01-Jan-10\"\n\"Derived EIA Weekly Estimates\"\n769\n900\n195\n268\n985\n159\n826\n2010-01-01\n\n\n\"08-Jan-10\"\n\"Derived EIA Weekly Estimates\"\n703\n820\n185\n257\n886\n123\n763\n2010-01-08\n\n\n\"15-Jan-10\"\n\"Derived EIA Weekly Estimates\"\n642\n750\n176\n246\n793\n91\n702\n2010-01-15\n\n\n\"22-Jan-10\"\n\"Derived EIA Weekly Estimates\"\n616\n710\n171\n235\n789\n102\n687\n2010-01-22\n\n\n\"29-Jan-10\"\n\"Derived EIA Weekly Estimates\"\n582\n661\n164\n221\n779\n108\n671\n2010-01-29\n\n\n\n\n\n\nHere is what storage dynamic look like:\n\nsns.lineplot(ng_storage_data, x=\"date\", y=\"storage\")\n\n\n\n\n\n\n\n\nFinally, joining both frames into one.\n\ngas_storage_data = gas_data.join(\n    ng_storage_data[\"date\", \"storage\"], how=\"left\", on=\"date\"\n).with_columns(pl.col(\"storage\").forward_fill())\ngas_storage_data.head()\n\n\nshape: (5, 4)\n\n\n\ndate\nprice\nret\nstorage\n\n\ndate\nf64\nf64\ni64\n\n\n\n\n2011-04-01\n4.32\nnull\n578\n\n\n2011-04-04\n4.21\n0.974537\n578\n\n\n2011-04-05\n4.22\n1.002375\n578\n\n\n2011-04-06\n4.17\n0.988152\n578\n\n\n2011-04-07\n4.12\n0.98801\n578\n\n\n\n\n\n\nI use my feature engineer class to extract the following features:\n\nLog-return\nLog-variance\nStorage and variance quantiles\nStorage quantile derivative\n\nI use quantiles pretty much everywhere, because it is good way to standardise the data and still leave a lot of useful information in the transformed variable. If the value is 0.99, you know it’s a rare event, which is useful for debugging.\n\nfeature_engineer = FeatureEngineer(\n    drop_nulls=False,\n    transforms=[\n        LogReturn(source_field=\"ret\", feature_name=\"log_ret\"),\n        Variance(\n            source_field=\"price\",\n            feature_name=\"var\",\n            rolling_variance_window=2,\n        ),\n        LogReturn(source_field=\"var\", feature_name=\"log_var\", requested_lag=0),\n        QuantileTransformer(\n            source_field=\"var\", feature_name=\"var_quantile\", requested_lag=1\n        ),\n        QuantileTransformer(\n            source_field=\"storage\",\n            feature_name=\"storage_quantile\",\n            step_size=8,\n            requested_lag=1,\n        ),\n        Derivative(\n            source_field=\"storage_quantile\",\n            feature_name=\"storage_quantile_derivative\",\n            step_size=8,\n            requested_lag=1,\n        ),\n    ],\n    n_shifts=3,\n)\ndf_with_features = feature_engineer.create_features(gas_storage_data)\ndf_with_features = df_with_features.drop_nulls()\ndf_with_features\n\n\nshape: (3_484, 20)\n\n\n\ndate\nprice\nret\nstorage\nlog_ret\nvar\nlog_var\nvar_quantile\nstorage_quantile\nstorage_quantile_derivative\nmonth\nprev_log_ret_1\nprev_log_ret_2\nprev_log_ret_3\nprev_var_1\nprev_var_2\nprev_var_3\nprev_var_quantile_1\nprev_storage_quantile_1\nprev_storage_quantile_derivative_1\n\n\ndate\nf64\nf64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\ni8\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2011-04-26\n4.32\n0.988558\n610\n-0.011508\n0.00125\n-6.684612\n0.470952\n0.22973\n0.027528\n3\n0.009195\n0.0\n0.032867\n0.0008\n0.0001\n0.0098\n0.411746\n0.202202\n0.019019\n\n\n2011-04-27\n4.35\n1.006944\n610\n0.00692\n0.00045\n-7.706263\n0.340426\n0.22973\n0.027528\n3\n-0.011508\n0.009195\n0.0\n0.00125\n0.0008\n0.0001\n0.470952\n0.202202\n0.019019\n\n\n2011-04-28\n4.38\n1.006897\n610\n0.006873\n0.00045\n-7.706263\n0.340574\n0.22973\n0.0\n3\n0.00692\n-0.011508\n0.009195\n0.00045\n0.00125\n0.0008\n0.340426\n0.22973\n0.046547\n\n\n2011-04-29\n4.51\n1.02968\n640\n0.029248\n0.00845\n-4.773589\n0.770854\n0.264765\n0.035035\n3\n0.006873\n0.00692\n-0.011508\n0.00045\n0.00045\n0.00125\n0.340574\n0.22973\n0.046547\n\n\n2011-05-02\n4.6\n1.019956\n640\n0.019759\n0.00405\n-5.509038\n0.658659\n0.264765\n0.035035\n4\n0.029248\n0.006873\n0.00692\n0.00845\n0.00045\n0.00045\n0.770854\n0.22973\n0.046547\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2025-02-04\n3.25\n0.984848\n638\n-0.015267\n0.00125\n-6.684612\n0.47869\n0.261762\n-0.200701\n1\n0.11892\n-0.062831\n-0.080043\n0.06845\n0.01805\n0.0338\n0.951502\n0.462462\n-0.115616\n\n\n2025-02-05\n3.22\n0.990769\n638\n-0.009274\n0.00045\n-7.706263\n0.347916\n0.261762\n-0.074074\n1\n-0.015267\n0.11892\n-0.062831\n0.00125\n0.06845\n0.01805\n0.47869\n0.335836\n-0.242242\n\n\n2025-02-06\n3.31\n1.02795\n638\n0.027567\n0.00405\n-5.509038\n0.662417\n0.261762\n-0.074074\n1\n-0.009274\n-0.015267\n0.11892\n0.00045\n0.00125\n0.06845\n0.347916\n0.335836\n-0.242242\n\n\n2025-02-07\n3.32\n1.003021\n624\n0.003017\n0.0001\n-9.21034\n0.0\n0.241742\n-0.094094\n1\n0.027567\n-0.009274\n-0.015267\n0.00405\n0.00045\n0.00125\n0.662417\n0.335836\n-0.242242\n\n\n2025-02-10\n3.48\n1.048193\n624\n0.047068\n0.0128\n-4.35831\n0.831581\n0.241742\n-0.094094\n1\n0.003017\n0.027567\n-0.009274\n0.0001\n0.00405\n0.00045\n0.0\n0.335836\n-0.242242\n\n\n\n\n\n\n\n# Cut-off point\nT = 300\n(\n    present_value_log_ret,\n    present_value_log_var,\n    storage,\n    month,\n    present_value_test,\n) = (\n    df_with_features[\"log_ret\"][:-T].to_numpy(),\n    df_with_features[\"log_var\"][:-T].to_numpy(),\n    df_with_features[\"storage_quantile\"][:-T].to_numpy(),\n    df_with_features[\"month\"][:-T].to_numpy(),\n    df_with_features[\"log_ret\"][-T:].to_numpy(),\n)\npast_values_train, past_values_test = (\n    feature_engineer.to_numpy_dict(df_with_features[:-T]),\n    feature_engineer.to_numpy_dict(df_with_features[-T:]),\n)",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#model-definition",
    "href": "volatility_and_storage.html#model-definition",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Model definition",
    "text": "Model definition\nThe model is defined in the same way it was described above.\n\n\ngarch_like_sample_vol_model\n\n garch_like_sample_vol_model (value_log_ret:numpy.ndarray[typing.Any,numpy\n                              .dtype[+_ScalarType_co]]=None, log_var_value\n                              :numpy.ndarray[typing.Any,numpy.dtype[+_Scal\n                              arType_co]]=None, storage:numpy.ndarray[typi\n                              ng.Any,numpy.dtype[+_ScalarType_co]]=None, m\n                              onth:numpy.ndarray[typing.Any,numpy.dtype[+_\n                              ScalarType_co]]=None, past_values:dict[str,n\n                              umpy.ndarray[typing.Any,numpy.dtype[+_Scalar\n                              Type_co]]]=None)\n\n\nnumpyro.render_model(\n    garch_like_sample_vol_model,\n    model_args=(\n        present_value_log_ret,\n        present_value_log_var,\n        storage,\n        month,\n        past_values_train,\n    ),\n)\n\n\n\n\n\n\n\n\nThe first order of business is to do PPC - prior predictive checks. In out case, I deal with data in the temporal domain, hence our checks will be autoregressive too. What I mainly want to check here is that with the current prior I do not get explosive time series, and they stay within a reasonable level. On the image below, you can see, that this is indeed the case.\nThough PPCs are very useful in general, practically speaking, if you have a lot of observations, bad prior will be overcome by those data. This is the case for this model too: I tried to set unreasonable priors and the model would converge to the same values again and again. That’s because there are thousands of samples. Nonetheless, it is a good practice and I do it.\n\nsimulator = Simulator(\n    model=garch_like_sample_vol_model,\n    feature_engineer=feature_engineer,\n    exo_fixed_effects=[\"month\"],\n    inherit_vals=[\"storage\"],\n)\nstarting_sim_df = gas_storage_data[-T - feature_engineer.n_shifts * 5 - 100 : -T]\nall_runs = simulator.simulate_paths(\n    steps=7,\n    starting_sim_df=starting_sim_df,\n    posterior_samples=None,\n    num_sims=10,\n)\n\nsns.lineplot(all_runs, x=\"date\", y=\"price\", hue=\"run_id\")\nplt.xticks(rotation=90);",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#fitting-and-diagnostics",
    "href": "volatility_and_storage.html#fitting-and-diagnostics",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Fitting and diagnostics",
    "text": "Fitting and diagnostics\nThe model is fitted using MCMC. You can see that estimates are relatively tight. Note, how the derivative is entering with the negative sign into the linear equation. If the storage level is dropping(negative change), the term will make a positive contribution into the volatility.\nI also show the MCMC diagnostics below. There is a decent number of samples with low autocorrelations and the chains look to be mixing well.\n\nrng_key = random.PRNGKey(0)\nrng_key, rng_key_ = random.split(rng_key)\n\n# Run NUTS.\nkernel = NUTS(garch_like_sample_vol_model)\nnum_samples = 2000\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=num_samples)\n\nmcmc.run(\n    rng_key_,\n    present_value_log_ret,\n    present_value_log_var,\n    storage,\n    month,\n    past_values_train,\n)\nmcmc.print_summary()\nposterior_samples = mcmc.get_samples()\nprint(posterior_samples.keys())\n\nsample: 100%|██████████| 3000/3000 [00:01&lt;00:00, 1742.73it/s, 7 steps of size 2.36e-01. acc. prob=0.93] \n\n\n\n                                                mean       std    median      5.0%     95.0%     n_eff     r_hat\n                                     b_var     -7.11      0.00     -7.11     -7.12     -7.10   1296.73      1.00\n             param_prev_storage_quantile_1      0.19      0.01      0.19      0.18      0.20   1335.46      1.00\n  param_prev_storage_quantile_derivative_1     -3.25      0.02     -3.25     -3.29     -3.21   1161.86      1.00\n                 param_prev_var_quantile_1      1.23      0.01      1.23      1.22      1.24   1393.80      1.00\n\nNumber of divergences: 0\ndict_keys(['b_var', 'param_prev_storage_quantile_1', 'param_prev_storage_quantile_derivative_1', 'param_prev_var_quantile_1'])\n\n\n\n\n\n\nposterior_samples = mcmc.get_samples()\npredictive = Predictive(\n    garch_like_sample_vol_model,\n    posterior_samples=posterior_samples,\n    return_sites=[\"log_ret\"],  # or whatever your observation site is called\n)\nrng_key, rng_key_ppc = random.split(rng_key)\nppc_samples = predictive(\n    rng_key_ppc,\n    present_value_log_ret,\n    present_value_log_var,\n    storage,\n    month,\n    past_values_train,\n)\nprior_samples = Predictive(garch_like_sample_vol_model, num_samples=2000)(\n    rng_key,\n    present_value_log_ret,\n    present_value_log_var,\n    storage,\n    month,\n    past_values_train,\n)\n\nidata = az.from_numpyro(mcmc, posterior_predictive=ppc_samples, prior=prior_samples)\naz.plot_trace(idata, var_names=[\"~^log_ret_var\"], filter_vars=\"regex\");\n\n/Users/sofeikov/work/bayesianfin/.venv/lib/python3.12/site-packages/arviz/utils.py:146: UserWarning: Items starting with ~: ['^log_ret_var'] have not been found and will be ignored\n  warnings.warn(",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#autoregressive-quality-checks",
    "href": "volatility_and_storage.html#autoregressive-quality-checks",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Autoregressive quality checks",
    "text": "Autoregressive quality checks\nIn this section I show how to check the quality of the model in this autoregressive setup. First I model a few dozens paths to see if on average the predictions stays within reasonable bounds. It is followed by inclusion error checks - model a few points in the future, and see where does an actual price lies within your prediction range.\n\nsimulator = Simulator(\n    model=garch_like_sample_vol_model,\n    feature_engineer=feature_engineer,\n    exo_fixed_effects=[\"month\"],\n    inherit_vals=[\"storage\"],\n)\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\nstarting_sim_df = gas_storage_data[-T - feature_engineer.n_shifts * 5 - 50 : -T]\nall_runs = simulator.simulate_paths(\n    steps=30,\n    starting_sim_df=starting_sim_df,\n    posterior_samples=None,\n    num_sims=100,\n)\n\nAs you can see on the image below, the paths look quite reasonable, with few deviating too much. This was, of course, a qualitative statement, rather than a quantitative one. But the latter is coming too.\n\nsns.lineplot(all_runs, x=\"date\", y=\"price\", hue=\"run_id\")\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\n\nlast_prices = (\n    all_runs.sort(\"date\")  # Ensure dates are in correct order\n    .group_by(\"run_id\")\n    .agg(pl.col(\"price\").last())  # Get the last price for each run\n    .rename({\"price\": \"final_price\"})\n)\nsns.histplot(last_prices, x=\"final_price\")",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#inclusion-error-checks",
    "href": "volatility_and_storage.html#inclusion-error-checks",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Inclusion error checks",
    "text": "Inclusion error checks\nHere I do some inclusion checks to see how well the actual prices fit into the price spread predicted by the model.\n\ntarget_window = 7\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\nshifts = list(range(0, T, 10))\nall_lasts = []\nactual_prices = []\nfor ct, t in tqdm(enumerate(shifts), total=len(shifts)):\n    starting_sim_df = gas_storage_data[\n        -T - feature_engineer.n_shifts * 5 - 50 + t : -T + t\n    ]\n    this_sim = simulator.simulate_paths(\n        steps=target_window,\n        starting_sim_df=starting_sim_df,\n        posterior_samples=posterior_for_gen,\n        num_sims=50,\n    )\n    last_prices = (\n        this_sim.sort(\"date\")  # Ensure dates are in correct order\n        .group_by(\"run_id\")\n        .agg(pl.col(\"price\").last())  # Get the last price for each run\n        .rename({\"price\": \"final_price\"})\n        .with_columns(pl.lit(ct).alias(\"run_id\"))\n    )\n    actual_prices.append(\n        {\"run_id\": ct, \"actual_price\": gas_storage_data[-T + t][\"price\"][0]}\n    )\n    all_lasts.append(last_prices)\nall_lasts = pl.concat(all_lasts)\nactual_prices = pl.DataFrame(actual_prices)\n\n100%|██████████| 30/30 [00:31&lt;00:00,  1.04s/it]\n\n\nFrom the image below it can be seen that all observed prices, except for one(which was a huge jump in real data) lie within the predicted interval. However it is always interesting to compare it with another version of the model. Let’s do that for the model that does not include storage as a regressor. Here is the result:\n\n\n\nimage.png\n\n\nIf you compare this image with the image below, you can clearly see that there are less outliers in the box plot, which means that the storage information that I added lead to improved precision and prediction stability. Which means that the storage information does indeed contain a useful signal. Practically speaking, if you model does not overprice the commodity value too much, it means that you can offer more competitive prices on the derivatives you might be selling.\n\nsns.boxplot(data=all_lasts, x=\"run_id\", y=\"final_price\")\nsns.scatterplot(data=actual_prices, x=\"run_id\", y=\"actual_price\", zorder=10)\n\n\n\n\n\n\n\n\nThe observed tighter spread above is confirmed by a lower error(0.065 vs 0.087).\n\nall_lasts.join(\n    actual_prices, left_on=\"run_id\", right_on=\"run_id\", how=\"inner\"\n).with_columns(error=(pl.col(\"final_price\") - pl.col(\"actual_price\")) ** 2).group_by(\n    \"run_id\"\n).agg(pl.col(\"error\").mean())[\"error\"].median()\n\n0.06586484831089998",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#conclusion",
    "href": "volatility_and_storage.html#conclusion",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Conclusion",
    "text": "Conclusion\nBy including storage information as regressors I was able to get a better model that has a better predictive power and more realistic volatility dynamics. By finally using macro regressors like storage levels, I finally get to work on a model that is somewhat unique to gas and it can reflect that market’s specific dynamics.\nDespite improving precision, this model suffers still suffers from the following:\n\nThe vol window is too short in the feature engineering\nThe volatility-of-volatility is currently fixed at a very low number. So far I could not find a satisfactory way to model it, it just leads to volatility explosion\nThe seasonal effects are not included. Given that on winter months storages are depleted, while being injected in the summer, this should explain a good deal of volatility. However, this introduces 12 additional parameters and it is unclear how use them. As a bias entry into volatility? As a vol-of-vol? As something affecting the storage levels?\n\nSo far adding seasonality effects has led to worse sampling efficiency in the MCMC, and while prediction still look good, they do not improve, which feels wrong. Stay tuned — tackling seasonality is my next move.",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "volatility_and_storage.html#additional-reading",
    "href": "volatility_and_storage.html#additional-reading",
    "title": "Using storage information to model Henry Hub Gas prices",
    "section": "Additional reading",
    "text": "Additional reading\n\nUnderstanding Henry Hub\nNatural gas seasonality and storage\nWeekly gas storage report",
    "crumbs": [
      "Using storage information to model Henry Hub Gas prices"
    ]
  },
  {
    "objectID": "00_data.html",
    "href": "00_data.html",
    "title": "bayesianfin",
    "section": "",
    "text": "source\n\nDataLoader\n\n DataLoader (max_records:int=None)\n\n*A class for loading and processing time series data with adjustable parameters.\nThis class handles loading CSV data, computing returns, and calculating rolling variance.\nAttributes: max_records: Maximum number of records to keep (from the end of the dataset)*\n\n# Create a data loader with default parameters and load the data\ndata_loader = DataLoader(max_records=9000)\nsource_df = data_loader.load_data(\"./data/ng_daily.csv\")\nsource_df.head()\n\n\nshape: (5, 3)\n\n\n\ndate\nprice\nret\n\n\ndate\nf64\nf64\n\n\n\n\n1997-01-07\n3.82\nnull\n\n\n1997-01-08\n3.8\n0.994764\n\n\n1997-01-09\n3.61\n0.95\n\n\n1997-01-10\n3.92\n1.085873\n\n\n1997-01-13\n4.0\n1.020408\n\n\n\n\n\n\n\nsource\n\n\nFeatureEngineer\n\n FeatureEngineer (transforms:list[__main__.DFFeature], n_shifts=3,\n                  drop_nulls:bool=True)\n\n*A class for creating lagged features from time series data.\nThis class handles the creation of lagged (shifted) features that can be used for GARCH-like models and other time series forecasting tasks.\nAttributes: columns: List of column names to create lags for n_shifts: Number of lag periods to create drop_nulls: whether to drop the nulls after rolling window calculations*\n\nsource\n\n\nDerivative\n\n Derivative (source_field:str, feature_name:str,\n             requested_lag:int|None=None, step_size:int=1)\n\n\nsource\n\n\nIdentity\n\n Identity (source_field:str, feature_name:str,\n           requested_lag:int|None=None, step_size:int=1)\n\n\nsource\n\n\nLogReturn\n\n LogReturn (source_field:str, feature_name:str,\n            requested_lag:int|None=None, step_size:int=1)\n\n\nsource\n\n\nSquare\n\n Square (source_field:str, feature_name:str, requested_lag:int|None=None,\n         step_size:int=1)\n\n\nsource\n\n\nVariance\n\n Variance (source_field:str, feature_name:str,\n           requested_lag:int|None=None, step_size:int=1,\n           rolling_variance_window:int=3)\n\n\nsource\n\n\nQuantileTransformer\n\n QuantileTransformer (source_field:str, feature_name:str,\n                      requested_lag:int|None=None, step_size:int=1,\n                      n_quantiles:int=1000,\n                      output_distribution:str='uniform')\n\nA stateful quantile transformer using sklearn’s QuantileTransformer.\n\nsource\n\n\nZeroBasedMonth\n\n ZeroBasedMonth (source_field:str, feature_name:str,\n                 requested_lag:int|None=None, step_size:int=1)\n\n\nsource\n\n\nDFFeature\n\n DFFeature (source_field:str, feature_name:str,\n            requested_lag:int|None=None, step_size:int=1)\n\n\nfeature_engineer = FeatureEngineer(\n    transforms=[\n        LogReturn(source_field=\"ret\", feature_name=\"log_ret\"),\n        Variance(source_field=\"price\", feature_name=\"var\", requested_lag=0),\n        QuantileTransformer(\n            source_field=\"var\", feature_name=\"var_quantile\", requested_lag=0\n        ),\n    ],\n    n_shifts=3,\n)\ndf_with_features = feature_engineer.create_features(source_df)\ndf_with_features.head()\n\n\nshape: (5, 9)\n\n\n\ndate\nprice\nret\nlog_ret\nvar\nvar_quantile\nprev_log_ret_1\nprev_log_ret_2\nprev_log_ret_3\n\n\ndate\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1997-01-13\n4.0\n1.020408\n0.020203\n0.042433\n0.860861\n0.082384\n-0.051293\n-0.005249\n\n\n1997-01-14\n4.01\n1.0025\n0.002497\n0.002433\n0.316111\n0.020203\n0.082384\n-0.051293\n\n\n1997-01-15\n4.34\n1.082294\n0.079083\n0.037433\n0.847429\n0.002497\n0.020203\n0.082384\n\n\n1997-01-16\n4.71\n1.085253\n0.081814\n0.122633\n0.944662\n0.079083\n0.002497\n0.020203\n\n\n1997-01-17\n3.91\n0.830149\n-0.186151\n0.1603\n0.95773\n0.081814\n0.079083\n0.002497\n\n\n\n\n\n\n\nqt = QuantileTransformer(source_field=\"price\", feature_name=\"price_quantile\")\nqt.fit(source_df)\nsource_df = qt.extract(source_df)\nsource_df.head()\n\n\nshape: (5, 4)\n\n\n\ndate\nprice\nret\nprice_quantile\n\n\ndate\nf64\nf64\nf64\n\n\n\n\n1997-01-07\n3.82\nnull\n0.573073\n\n\n1997-01-08\n3.8\n0.994764\n0.569069\n\n\n1997-01-09\n3.61\n0.95\n0.535536\n\n\n1997-01-10\n3.92\n1.085873\n0.591091\n\n\n1997-01-13\n4.0\n1.020408\n0.607107\n\n\n\n\n\n\n\nsource\n\n\nappend_from_log_ret\n\n append_from_log_ret (df:polars.dataframe.frame.DataFrame,\n                      new_log_ret:float, inherit_vals:list[str],\n                      add_variables:dict[str,float])\n\n*Adds a new record to the dataframe based on a log return value.\nArgs: df: Input DataFrame containing time series data new_log_ret: The new log return value to add\nReturns: DataFrame with a new row appended*\n\nsource\n\n\nbinary_feature_from_date_ranges\n\n binary_feature_from_date_ranges\n                                  (date_range:tuple[datetime.date,datetime\n                                  .date], periods:list[tuple[datetime.date\n                                  ,datetime.date]],\n                                  feature_name:str='feature')\n\n\nbinary_feature_from_date_ranges(\n    date_range=(date(2010, 1, 1), date(2026, 1, 1)),\n    periods=[\n        (date(2022, 2, 24), date(2026, 1, 1)),\n    ],\n    feature_name=\"RU/UA_war\",\n)\n\n\nshape: (5_845, 2)\n\n\n\ndate\nRU/UA_war\n\n\ndate\ni64\n\n\n\n\n2010-01-01\n0\n\n\n2010-01-02\n0\n\n\n2010-01-03\n0\n\n\n2010-01-04\n0\n\n\n2010-01-05\n0\n\n\n…\n…\n\n\n2025-12-28\n1\n\n\n2025-12-29\n1\n\n\n2025-12-30\n1\n\n\n2025-12-31\n1\n\n\n2026-01-01\n1",
    "crumbs": [
      "00_data.html"
    ]
  },
  {
    "objectID": "garch_like_modelling.html",
    "href": "garch_like_modelling.html",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "",
    "text": "I built a Bayesian GARCH-like model for heating oil prices using NumPyro. Here’s the full messy journey. Also, as if it was not clear, never ever attempt to use it for anything practical - this will not end well, so not a financial advice :D",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#tldr",
    "href": "garch_like_modelling.html#tldr",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "",
    "text": "I built a Bayesian GARCH-like model for heating oil prices using NumPyro. Here’s the full messy journey. Also, as if it was not clear, never ever attempt to use it for anything practical - this will not end well, so not a financial advice :D",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#introduction",
    "href": "garch_like_modelling.html#introduction",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "Introduction",
    "text": "Introduction\nI’ve been interested in financial markets for a while, especially the modeling failures surrounding the 2008 financial crisis. One recurring theme is how standard models struggled to capture risk during volatile periods. That led me to explore financial modeling more seriously, starting with option pricing as a fundamental use case.\nWhile it’s easy enough to plug numbers into Black-Scholes, I wanted to build something from the ground up—specifically a model that learns volatility structure directly from data. So I built a GARCH-inspired, fully Bayesian model using NumPyro that captures both return and variance dynamics.\nThis post walks through that process: loading and transforming market data, fitting the model, simulating future prices, and estimating option payoffs. It’s a basic framework, but a flexible one, and I’ve also started thinking about ways to extend it—for example, conditioning on storage levels data.\nUsing the model’s generative structure, I simulated forward price paths and estimated call option payoffs at expiration. From these, I computed the likelihood of finishing in the money, and compared the expected payoff to the cost of the option. If the average payoff exceeds the premium, it suggests a positive expected value—at least under the model’s assumptions.\nFor now, this assumes European-style options and a buy-and-hold strategy to maturity. The underlying commodity we are targeting isHeating Oil.",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#data-loading-process",
    "href": "garch_like_modelling.html#data-loading-process",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "Data loading process",
    "text": "Data loading process\nWe start by loading the data, and examining the returns, log-return distribution, as well as variance plots. The dataset we are using today contains heating oil prices from 2010 until May, 2025. The dataset was pulled from investing.com.\n\nsource_df = (\n    pl.read_csv(\n        \"./data/heating_oil.csv\",\n        infer_schema_length=0,\n        dtypes={\"Price\": pl.Utf8},\n        columns=[\"Date\", \"Price\"],\n    )\n    .with_columns(\n        pl.col(\"Date\").str.to_date(format=\"%m/%d/%Y\"),\n        pl.col(\"Price\").str.replace_all(\",\", \"\").cast(pl.Float64),\n    )\n    .rename({\"Date\": \"date\", \"Price\": \"price\"})\n    .with_columns(\n        ret=pl.col(\"price\") / pl.col(\"price\").shift(1),\n    )\n).sort(\"date\")\nsource_df\n\n/var/folders/8s/q13s1_m56g3b3k_1fmdrwn_80000gn/T/ipykernel_2363/3357814223.py:2: DeprecationWarning: The argument `dtypes` for `read_csv` is deprecated. It has been renamed to `schema_overrides`.\n  pl.read_csv(\n\n\n\nshape: (3_950, 3)\n\n\n\ndate\nprice\nret\n\n\ndate\nf64\nf64\n\n\n\n\n2010-04-05\n2.2825\n0.998775\n\n\n2010-04-06\n2.2853\n1.010703\n\n\n2010-04-07\n2.2611\n1.007396\n\n\n2010-04-08\n2.2445\n1.000401\n\n\n2010-04-09\n2.2436\n1.002771\n\n\n…\n…\n…\n\n\n2025-04-28\n2.1755\n1.025986\n\n\n2025-04-29\n2.1204\n1.041096\n\n\n2025-04-30\n2.0367\n1.015608\n\n\n2025-05-01\n2.0054\n1.001598\n\n\n2025-05-02\n2.0022\nnull\n\n\n\n\n\n\nThe class below is implemented to ease off the data analysis and handling. It produces lagged features for returns and variances, as well as transforms the data so it is easier to feed into a model.\n\nfeature_engineer = FeatureEngineer(\n    transforms=[\n        LogReturn(source_field=\"ret\", feature_name=\"log_ret\"),\n        Variance(\n            source_field=\"price\",\n            feature_name=\"var\",\n            rolling_variance_window=2,\n        ),\n        QuantileTransformer(source_field=\"var\", feature_name=\"var_quantile\"),\n    ],\n    n_shifts=3,\n)\ndf_with_features = feature_engineer.create_features(source_df)\ndf_with_features.head()\n\n\nshape: (5, 15)\n\n\n\ndate\nprice\nret\nlog_ret\nvar\nvar_quantile\nprev_log_ret_1\nprev_log_ret_2\nprev_log_ret_3\nprev_var_1\nprev_var_2\nprev_var_3\nprev_var_quantile_1\nprev_var_quantile_2\nprev_var_quantile_3\n\n\ndate\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2010-04-09\n2.2436\n1.002771\n0.002767\n0.0001\n0.0\n0.000401\n0.007369\n0.010646\n0.000138\n0.000293\n0.0001\n0.388388\n0.53155\n0.0\n\n\n2010-04-12\n2.2374\n1.001791\n0.001789\n0.0001\n0.0\n0.002767\n0.000401\n0.007369\n0.0001\n0.000138\n0.000293\n0.0\n0.388388\n0.53155\n\n\n2010-04-13\n2.2334\n0.987531\n-0.012547\n0.0001\n0.0\n0.001789\n0.002767\n0.000401\n0.0001\n0.0001\n0.000138\n0.0\n0.0\n0.388388\n\n\n2010-04-14\n2.2616\n0.994591\n-0.005424\n0.000398\n0.596285\n-0.012547\n0.001789\n0.002767\n0.0001\n0.0001\n0.0001\n0.0\n0.0\n0.0\n\n\n2010-04-15\n2.2739\n1.015406\n0.015288\n0.0001\n0.0\n-0.005424\n-0.012547\n0.001789\n0.000398\n0.0001\n0.0001\n0.596285\n0.0\n0.0",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#exploratory-data-analysis",
    "href": "garch_like_modelling.html#exploratory-data-analysis",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nLet’s look at various dynamics and distributions associated with this dataset. The first thing we notice is that neither return nor their log counterparts are distributed normally. Instead we see very narrow shapes with random variables that are sufficiently sub-gaussian. From my reading online it is quite normal for commodities, especially heating oil.\n\nsns.histplot(df_with_features, x=\"log_ret\")\nsns.histplot(df_with_features, x=\"ret\")\nplt.xlim([-1, 2])\n\n\n\n\n\n\n\n\nThe 3-day lag variance dynamics are plotted below. As we can see the variance has periods of massive volatility.\n\nplt.figure(figsize=(16, 6))\nsns.lineplot(df_with_features, x=\"date\", y=\"price\")\nplt.figure(figsize=(16, 6))\nsns.lineplot(df_with_features, x=\"date\", y=\"log_ret\")",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#the-model",
    "href": "garch_like_modelling.html#the-model",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "The model",
    "text": "The model\nI decided to go for the following process of modelling the log returns:\n\\[\n\\begin{align*}\n\\log r_t &\\sim \\mathcal{N}(0, \\sigma_t^2) \\\\\n\\sigma_t^2 &= \\beta_0 + \\sum_{i=1}^S \\beta_i \\sigma_{t-i}^2\n\\end{align*}\n\\]\nWhile this model does not properly model the variance the way a ARCH/GARCH model would, I’m going to go with it for the sake of this learning exercise. Note that the mean for this model is fixed and set to 0. The reason for this is that it’s a well known property of log returns to be dead centered around 0, so the only thing that matter is the volatility we are trying to predict.\nIt is also worth mentioning that there is a conceptual gap between me using the dataset variance for learning and later generating variance myself and essentially continuing in an autoregressive manner. While true, it’s practically a workable approach and used widely in other fields of ML, e.g. trajectory prediction with generative models. One of the next things I’m intending to do is to consider volatility to be latent variable and sample it myself, conditioning the site on observed volatility at the time. When it is not observed, it will just be sampled during the generative phase. In a way, we use variance as a noisy estimate of the latent variance, which, again, not how things are done properly, but that is a story for an upcoming article.\n\n# Cut-off point\nT = 300\npresent_value_train, present_value_test = (\n    df_with_features[\"log_ret\"][:-T].to_numpy(),\n    df_with_features[\"log_ret\"][-T:].to_numpy(),\n)\npast_values_train, past_values_test = (\n    feature_engineer.to_numpy_dict(df_with_features[:-T]),\n    feature_engineer.to_numpy_dict(df_with_features[-T:]),\n)\n\n\nsource\n\ngarch_like_model\n\n garch_like_model (present_value:numpy.ndarray[typing.Any,numpy.dtype[+_Sc\n                   alarType_co]]=None, past_values:dict[str,numpy.ndarray[\n                   typing.Any,numpy.dtype[+_ScalarType_co]]]=None)\n\nHere’s a visualization of the model structure to clarify the dependencies.\n\nnumpyro.render_model(\n    garch_like_model,\n    model_args=(\n        present_value_train,\n        past_values_train,\n    ),\n)\n\n\n\n\n\n\n\n\nThe model is fitted using MCMC with the NUTS sampler.\n\nrng_key = random.PRNGKey(0)\nrng_key, rng_key_ = random.split(rng_key)\n\n# Run NUTS.\nkernel = NUTS(garch_like_model)\nnum_samples = 2000\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=num_samples)\n\nmcmc.run(\n    rng_key_,\n    present_value=present_value_train,\n    past_values=past_values_train,\n)\nmcmc.print_summary()\n\nsample: 100%|██████████| 3000/3000 [00:17&lt;00:00, 171.17it/s, 603 steps of size 2.54e-03. acc. prob=0.94] \n\n\n\n                                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n                      b_var      0.00      0.00      0.00      0.00      0.00   1153.03      1.00\n  param_prev_var_quantile_1      0.00      0.00      0.00      0.00      0.00   1394.41      1.00\n  param_prev_var_quantile_2      0.00      0.00      0.00      0.00      0.00    674.56      1.00\n  param_prev_var_quantile_3      0.00      0.00      0.00      0.00      0.00    618.45      1.00\n\nNumber of divergences: 0\n\n\nWith the model converged to good values, with great values for \\(\\hat{r}\\), we should take a look at Bayesian factors. In this case we realise that for the volatility regression coefficients, we get values well above 1, suggesting there is enough evidence to reject the prior.\n\nposterior_samples = mcmc.get_samples()\npredictive = Predictive(\n    garch_like_model,\n    posterior_samples=posterior_samples,\n    return_sites=[\"log_ret\"],  # or whatever your observation site is called\n)\nrng_key, rng_key_ppc = random.split(rng_key)\nppc_samples = predictive(\n    rng_key_ppc, present_value=present_value_train, past_values=past_values_train\n)\nprior_samples = Predictive(garch_like_model, num_samples=2000)(\n    rng_key,\n    present_value=present_value_train,\n    past_values=past_values_train,\n)\n\nidata = az.from_numpyro(mcmc, posterior_predictive=ppc_samples, prior=prior_samples)\nfor k in posterior_samples.keys():\n    az.plot_bf(idata, var_name=k)\n    plt.gca().set_xlim([-0.0005, 0.0005])\n\narviz - WARNING - The reference value is outside of the posterior. This translate into infinite support for H1, which is most likely an overstatement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe model seems to be converging well(notice the \\(\\hat{r}\\)). Now let’s explore basic diagnostics for the model. Starting with the forest plot of the posterior. I can immediately notice two interesting things: both biases for log return and variance are 0. Also, all variance terms except for the previous one are essentially 0 too, meaning that only the variance of the previous 3 days is important. We might very well have selected a bad time frame and we need to experiment with shorter/longer variance aggregation time frames. There is a need to study volatility clustering patterns somehow.\nNote, how the estimated coefficients values are really small. This is not very surprising, since normally, the variance values are quite small, so past values the actual noisy measurements of variance need to be scaled appropriately to come with a good value for new volatility.\n\naz.plot_forest(posterior_samples, var_names=[\"~df\"])\n\n/Users/sofeikov/work/bayesianfin/.venv/lib/python3.12/site-packages/arviz/utils.py:146: UserWarning: Items starting with ~: ['df'] have not been found and will be ignored\n  warnings.warn(\n\n\narray([&lt;Axes: title={'center': '94.0% HDI'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nLet’s plot the HMC chains too, which all look adequate and as if sampling happens with high efficiency. The only odd chain is for the second lag of the variance, but that coefficient is estimated to be 0, which is consistent with higher order lags, so it seems like it can be safely ignored.\n\naz.plot_trace(idata);\n\n\n\n\n\n\n\n\nFinally ready to generate some paths for the oil prices. This is a simple autoregressive loop wherein the newly generated data is fed as input into the next step.\n\nsimulator = Simulator(\n    model=garch_like_model,\n    feature_engineer=feature_engineer,\n)\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\nstarting_sim_df = source_df[-T - feature_engineer.n_shifts * 5 - 50 : -T]\nall_runs = simulator.simulate_paths(\n    steps=30,\n    starting_sim_df=starting_sim_df,\n    posterior_samples=posterior_for_gen,\n    num_sims=100,\n)\nsns.lineplot(all_runs, x=\"date\", y=\"price\", hue=\"run_id\")\nplt.xticks(rotation=90);\n\n\n\n\n\n\n\n\nAs we can see on the charts below, the generated paths exhibit generally adequate behavior. Very few paths deviate and end up too high, and there is no degenerate behaviour with negative prices or prices near 0. While negative prices did happen in 2022 for oil, that is not typical and not something one could reasonably predict.\nI now turn my attention to looking at the distribution for the final prices among the generated paths. We can see a somewhat healthy asymmetric distribution with a few high price examples on the right hand side. We can clearly see that the paths generated show there is some reasonable expected price behavior.",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "garch_like_modelling.html#some-backtesting",
    "href": "garch_like_modelling.html#some-backtesting",
    "title": "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing",
    "section": "Some backtesting",
    "text": "Some backtesting\nI now turn my attention to testing the predictive power of the model. Posterior checks are good and fun, but the model is autoregressive in nature, so what needs to happen is we need to roll through out test set, simulate a distribution of forward looking prices, then do boxplots, with the predicted price distributions and the actual price inside those boxes. The purpose of this exercise is to see if the actual price tends to be within the range of prices that comes out of simulations.\nIn a no volatility perfectly almost perfectly predictable world, the boxes would be very tight around the actual observations, but realistically there is going to be a lot of wiggle room inside.\n\ntarget_window = 30\nsimulator = Simulator(model=garch_like_model, feature_engineer=feature_engineer)\nposterior_for_gen = {k: ps[0:1] for k, ps in posterior_samples.items()}\nshifts = list(range(0, T, 10))\nall_lasts = []\nactual_prices = []\nfor ct, t in tqdm(enumerate(shifts), total=len(shifts)):\n    # print(f\"Simulating shift {ct}/{len(shifts)}\")\n    starting_sim_df = source_df[-T - feature_engineer.n_shifts * 5 - 50 + t : -T + t]\n    this_sim = simulator.simulate_paths(\n        steps=target_window,\n        starting_sim_df=starting_sim_df,\n        posterior_samples=posterior_for_gen,\n        num_sims=50,\n    )\n    last_prices = (\n        this_sim.sort(\"date\")  # Ensure dates are in correct order\n        .group_by(\"run_id\")\n        .agg(pl.col(\"price\").last())  # Get the last price for each run\n        .rename({\"price\": \"final_price\"})\n        .with_columns(pl.lit(ct).alias(\"run_id\"))\n    )\n    actual_prices.append({\"run_id\": ct, \"actual_price\": source_df[-T + t][\"price\"][0]})\n    all_lasts.append(last_prices)\nall_lasts = pl.concat(all_lasts)\nactual_prices = pl.DataFrame(actual_prices)\n\n100%|██████████| 30/30 [01:26&lt;00:00,  2.87s/it]\n\n\n\nsns.boxplot(data=all_lasts, x=\"run_id\", y=\"final_price\")\nsns.scatterplot(data=actual_prices, x=\"run_id\", y=\"actual_price\", zorder=10)\n# plt.ylim([0, 200])\n\n\n\n\n\n\n\n\nCalculate average deviation from target\n\nall_lasts.join(\n    actual_prices, left_on=\"run_id\", right_on=\"run_id\", how=\"left\"\n).with_columns(error=(pl.col(\"final_price\") - pl.col(\"actual_price\")) ** 2).group_by(\n    \"run_id\"\n).agg(pl.col(\"error\").mean())[\"error\"].mean()\n\n0.08656773378088618\n\n\n\nlast_prices = (\n    all_runs.sort(\"date\")  # Ensure dates are in correct order\n    .group_by(\"run_id\")\n    .agg(pl.col(\"price\").last())  # Get the last price for each run\n    .rename({\"price\": \"final_price\"})\n)\nsns.histplot(last_prices, x=\"final_price\")\n\n\n\n\n\n\n\n\nNext, we calculate the call option payoff distributions, as well as model-implied option prices.\n\nstrike_prices = [2.5, 3.0, 3.5, 4.0]\n\n# Create a list of DataFrames, one per strike\npayoff_dfs = [\n    last_prices.with_columns(\n        payoff_at_expiry=(pl.col(\"final_price\") - strike).clip(0),\n        strike=pl.lit(strike),  # so we can track which row belongs to which strike\n    )\n    for strike in strike_prices\n]\n\n# Concatenate into one big DataFrame\npayoff_df_all = pl.concat(payoff_dfs)\nplt.figure(figsize=(12, 6))\nsns.histplot(\n    data=payoff_df_all,\n    x=\"payoff_at_expiry\",\n    hue=\"strike\",\n    element=\"step\",\n    stat=\"count\",\n    bins=40,\n)\nplt.title(\"Call Option Payoff Distribution at Expiry\")\nplt.xlabel(\"Payoff at Expiry\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group and average payoff per strike\nimplied_prices = payoff_df_all.group_by(\"strike\").agg(pl.col(\"payoff_at_expiry\").mean())\n\n# Plot the pricing curve\nplt.figure(figsize=(10, 5))\nsns.lineplot(data=implied_prices, x=\"strike\", y=\"payoff_at_expiry\", marker=\"o\")\nplt.title(\"Model-Implied Call Option Prices vs Strike\")\nplt.xlabel(\"Strike Price\")\nplt.ylabel(\"Fair Price (Expected Payoff)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis wraps up my initial attempt at a generative, volatility-aware option pricing framework. There’s more work to be done—especially around incorporating latent volatility, or testing market-implied vol surfaces—but even this simplified model gives useful directional insight. Overall, this project was a small but useful step toward building generative models that respect volatility dynamics, rather than assuming them away.There’s a long road between reasonable simulations and production-grade financial modeling, but this framework makes experimentation straightforward—which is exactly the point. Of course, this is an extremely simplified toy model and not meant for real-world trading or production.",
    "crumbs": [
      "A Bayesian GARCH-Inspired Volatility Model for Energy Commodities Option Pricing"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applications of bayesian modelling in finance",
    "section": "",
    "text": "This project explores a generative approach to pricing natural gas options using a Bayesian GARCH-inspired model. It uses NumPyro to model volatility dynamics from historical data, simulates future prices, and estimates option payoffs under different strike scenarios.\nIt’s not a production-grade system, but a sandbox to understand how volatility-aware models behave—and how they might be extended further.\nI decided to go for the following process of modelling the log returns:\n\\[\n\\begin{align*}\n\\log r_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t^2) \\\\\n\\mu_t &= \\alpha_0 + \\sum_{i=1}^S \\alpha_i \\log r_{t-i} \\\\\n\\sigma_t^2 &= \\beta_0 + \\sum_{i=1}^S \\beta_i \\sigma_{t-i}^2\n\\end{align*}\n\\]\n\n📊 Loads and engineers features from natural gas price time series\n⚙️ Bayesian model for log returns and autoregressive variance\n🔁 Generates future price paths via simulation\n💰 Computes call option payoffs & model-implied prices\n🔍 Fully implemented in NumPyro with MCMC inference\n\n\n\n\n\n\npricepath.png\n\n\n\n\n\n\n\n\ncallpayoffatexpiry.png\n\n\n\n\n\n\n\n\nmodel-implied-price.png",
    "crumbs": [
      "Applications of bayesian modelling in finance"
    ]
  },
  {
    "objectID": "index.html#garch-like-model",
    "href": "index.html#garch-like-model",
    "title": "Applications of bayesian modelling in finance",
    "section": "",
    "text": "This project explores a generative approach to pricing natural gas options using a Bayesian GARCH-inspired model. It uses NumPyro to model volatility dynamics from historical data, simulates future prices, and estimates option payoffs under different strike scenarios.\nIt’s not a production-grade system, but a sandbox to understand how volatility-aware models behave—and how they might be extended further.\nI decided to go for the following process of modelling the log returns:\n\\[\n\\begin{align*}\n\\log r_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t^2) \\\\\n\\mu_t &= \\alpha_0 + \\sum_{i=1}^S \\alpha_i \\log r_{t-i} \\\\\n\\sigma_t^2 &= \\beta_0 + \\sum_{i=1}^S \\beta_i \\sigma_{t-i}^2\n\\end{align*}\n\\]\n\n📊 Loads and engineers features from natural gas price time series\n⚙️ Bayesian model for log returns and autoregressive variance\n🔁 Generates future price paths via simulation\n💰 Computes call option payoffs & model-implied prices\n🔍 Fully implemented in NumPyro with MCMC inference\n\n\n\n\n\n\npricepath.png\n\n\n\n\n\n\n\n\ncallpayoffatexpiry.png\n\n\n\n\n\n\n\n\nmodel-implied-price.png",
    "crumbs": [
      "Applications of bayesian modelling in finance"
    ]
  }
]