{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fd5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "import abc\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9908178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"A class for loading and processing time series data with adjustable parameters.\n",
    "\n",
    "    This class handles loading CSV data, computing returns, and calculating rolling variance.\n",
    "\n",
    "    Attributes:\n",
    "        max_records: Maximum number of records to keep (from the end of the dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_records: int = None):\n",
    "        self.max_records = max_records\n",
    "\n",
    "    def load_data(self, path: str) -> pl.DataFrame:\n",
    "        \"\"\"Load and process time series data from a CSV file.\n",
    "\n",
    "        Args:\n",
    "            path: Path to the CSV file containing the data\n",
    "\n",
    "        Returns:\n",
    "            Processed DataFrame with returns and rolling variance\n",
    "        \"\"\"\n",
    "        df = (\n",
    "            pl.read_csv(path, try_parse_dates=True, infer_schema_length=None)\n",
    "            .rename({\"Date\": \"date\", \"Price\": \"price\"})\n",
    "            .sort(\"date\")\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "            ret=pl.col(\"price\") / pl.col(\"price\").shift(1),\n",
    "        )\n",
    "\n",
    "        if self.max_records is not None:\n",
    "            df = df[-self.max_records :]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf477dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>price</th><th>ret</th></tr><tr><td>date</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1997-01-07</td><td>3.82</td><td>null</td></tr><tr><td>1997-01-08</td><td>3.8</td><td>0.994764</td></tr><tr><td>1997-01-09</td><td>3.61</td><td>0.95</td></tr><tr><td>1997-01-10</td><td>3.92</td><td>1.085873</td></tr><tr><td>1997-01-13</td><td>4.0</td><td>1.020408</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌────────────┬───────┬──────────┐\n",
       "│ date       ┆ price ┆ ret      │\n",
       "│ ---        ┆ ---   ┆ ---      │\n",
       "│ date       ┆ f64   ┆ f64      │\n",
       "╞════════════╪═══════╪══════════╡\n",
       "│ 1997-01-07 ┆ 3.82  ┆ null     │\n",
       "│ 1997-01-08 ┆ 3.8   ┆ 0.994764 │\n",
       "│ 1997-01-09 ┆ 3.61  ┆ 0.95     │\n",
       "│ 1997-01-10 ┆ 3.92  ┆ 1.085873 │\n",
       "│ 1997-01-13 ┆ 4.0   ┆ 1.020408 │\n",
       "└────────────┴───────┴──────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | exec: false\n",
    "# Create a data loader with default parameters and load the data\n",
    "data_loader = DataLoader(max_records=9000)\n",
    "source_df = data_loader.load_data(\"./data/ng_daily.csv\")\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DFFeature(abc.ABC):\n",
    "    source_field: str\n",
    "    feature_name: str\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Variance(DFFeature):\n",
    "    rolling_variance_window: int = 3\n",
    "\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(\n",
    "            pl.col(self.source_field)\n",
    "            .rolling_var(self.rolling_variance_window)\n",
    "            .clip(lower_bound=1e-4)\n",
    "            .alias(self.feature_name)\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Square(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(\n",
    "            (pl.col(self.source_field) ** 2).alias(self.feature_name)\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LogReturn(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(pl.col(self.source_field).log().alias(self.feature_name))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureEngineer:\n",
    "    \"\"\"A class for creating lagged features from time series data.\n",
    "\n",
    "    This class handles the creation of lagged (shifted) features that can be used for\n",
    "    GARCH-like models and other time series forecasting tasks.\n",
    "\n",
    "    Attributes:\n",
    "        columns: List of column names to create lags for\n",
    "        n_shifts: Number of lag periods to create\n",
    "        drop_nulls: whether to drop the nulls after rolling window calculations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transforms: list[DFFeature],\n",
    "        n_shifts=3,\n",
    "        drop_nulls: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize the FeatureEngineer.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to create lags for (default: ['ret', 'var'])\n",
    "            n_shifts: Number of lag periods to create (default: 3)\n",
    "            drop_nulls: whether to drop the nulls after rolling window calculations\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "        self.columns = [t.feature_name for t in transforms]\n",
    "        self.n_shifts = n_shifts\n",
    "        self.drop_nulls = drop_nulls\n",
    "\n",
    "    def create_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Create lagged features from the input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame containing time series data\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with original columns plus lagged features\n",
    "        \"\"\"\n",
    "        # Create a copy of the dataframe to avoid modifying the original\n",
    "        for t in self.transforms:\n",
    "            df = t.extract(df)\n",
    "\n",
    "        result_df = df.clone()\n",
    "\n",
    "        # Create lagged features for each specified column\n",
    "        for col in self.columns:\n",
    "            # Check if column exists in dataframe\n",
    "            if col not in df.columns:\n",
    "                print(f\"Warning: Column '{col}' not found in dataframe. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Create each lag\n",
    "            for shift in range(1, self.n_shifts + 1):\n",
    "                # Create new column name (e.g., prev_ret_1, prev_var_2)\n",
    "                new_col_name = f\"prev_{col}_{shift}\"\n",
    "\n",
    "                # Add the shifted column to the dataframe\n",
    "                result_df = result_df.with_columns(\n",
    "                    pl.col(col).shift(shift).alias(new_col_name)\n",
    "                )\n",
    "        if self.drop_nulls:\n",
    "            result_df = result_df.drop_nulls()\n",
    "        return result_df\n",
    "\n",
    "    def to_numpy_dict(self, df: pl.DataFrame, drop: set[str] | None = None) -> dict:\n",
    "        \"\"\"Convert the dataframe with lagged features to a dictionary of NumPy arrays.\n",
    "\n",
    "        This method extracts the original columns that were used to create lags,\n",
    "        as well as all the generated lag columns, and converts them to NumPy arrays.\n",
    "        The resulting dictionary can be used directly with NumPyro models.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with lagged features created by create_features()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping column names to NumPy arrays\n",
    "        \"\"\"\n",
    "        if drop is None:\n",
    "            drop = set()\n",
    "        # Create a new dataframe with the features we want to process\n",
    "        features_df = df.clone()\n",
    "\n",
    "        # Dictionary to store the NumPy arrays\n",
    "        numpy_dict = {}\n",
    "\n",
    "        # Add original columns\n",
    "        for col in self.columns:\n",
    "            if col in drop:\n",
    "                continue\n",
    "            if col in features_df.columns:\n",
    "                numpy_dict[col] = features_df[col].to_numpy()\n",
    "\n",
    "        # Add lagged features\n",
    "        for col in self.columns:\n",
    "            if col in drop:\n",
    "                continue\n",
    "            for shift in range(1, self.n_shifts + 1):\n",
    "                lag_col = f\"prev_{col}_{shift}\"\n",
    "                if lag_col in features_df.columns:\n",
    "                    numpy_dict[lag_col] = features_df[lag_col].to_numpy()\n",
    "\n",
    "        return numpy_dict\n",
    "\n",
    "    def get_iterator(self, site: str | None = None):\n",
    "        if site is None:\n",
    "            site = self.columns\n",
    "        else:\n",
    "            site = [site]\n",
    "        for pair in product(site, range(1, self.n_shifts + 1)):\n",
    "            yield pair\n",
    "\n",
    "    def get_shift_pattern(self, site: str, shift: int, prefix: str = \"\"):\n",
    "        return f\"{prefix}prev_{site}_{shift}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ef1014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>price</th><th>ret</th><th>log_ret</th><th>var</th><th>prev_log_ret_1</th><th>prev_log_ret_2</th><th>prev_log_ret_3</th><th>prev_var_1</th><th>prev_var_2</th><th>prev_var_3</th></tr><tr><td>date</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1997-01-14</td><td>4.01</td><td>1.0025</td><td>0.002497</td><td>0.002433</td><td>0.020203</td><td>0.082384</td><td>-0.051293</td><td>0.042433</td><td>0.024433</td><td>0.013433</td></tr><tr><td>1997-01-15</td><td>4.34</td><td>1.082294</td><td>0.079083</td><td>0.037433</td><td>0.002497</td><td>0.020203</td><td>0.082384</td><td>0.002433</td><td>0.042433</td><td>0.024433</td></tr><tr><td>1997-01-16</td><td>4.71</td><td>1.085253</td><td>0.081814</td><td>0.122633</td><td>0.079083</td><td>0.002497</td><td>0.020203</td><td>0.037433</td><td>0.002433</td><td>0.042433</td></tr><tr><td>1997-01-17</td><td>3.91</td><td>0.830149</td><td>-0.186151</td><td>0.1603</td><td>0.081814</td><td>0.079083</td><td>0.002497</td><td>0.122633</td><td>0.037433</td><td>0.002433</td></tr><tr><td>1997-01-20</td><td>3.26</td><td>0.83376</td><td>-0.18181</td><td>0.5275</td><td>-0.186151</td><td>0.081814</td><td>0.079083</td><td>0.1603</td><td>0.122633</td><td>0.037433</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌────────────┬───────┬──────────┬───────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ date       ┆ price ┆ ret      ┆ log_ret   ┆ … ┆ prev_log_r ┆ prev_var_1 ┆ prev_var_2 ┆ prev_var_ │\n",
       "│ ---        ┆ ---   ┆ ---      ┆ ---       ┆   ┆ et_3       ┆ ---        ┆ ---        ┆ 3         │\n",
       "│ date       ┆ f64   ┆ f64      ┆ f64       ┆   ┆ ---        ┆ f64        ┆ f64        ┆ ---       │\n",
       "│            ┆       ┆          ┆           ┆   ┆ f64        ┆            ┆            ┆ f64       │\n",
       "╞════════════╪═══════╪══════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ 1997-01-14 ┆ 4.01  ┆ 1.0025   ┆ 0.002497  ┆ … ┆ -0.051293  ┆ 0.042433   ┆ 0.024433   ┆ 0.013433  │\n",
       "│ 1997-01-15 ┆ 4.34  ┆ 1.082294 ┆ 0.079083  ┆ … ┆ 0.082384   ┆ 0.002433   ┆ 0.042433   ┆ 0.024433  │\n",
       "│ 1997-01-16 ┆ 4.71  ┆ 1.085253 ┆ 0.081814  ┆ … ┆ 0.020203   ┆ 0.037433   ┆ 0.002433   ┆ 0.042433  │\n",
       "│ 1997-01-17 ┆ 3.91  ┆ 0.830149 ┆ -0.186151 ┆ … ┆ 0.002497   ┆ 0.122633   ┆ 0.037433   ┆ 0.002433  │\n",
       "│ 1997-01-20 ┆ 3.26  ┆ 0.83376  ┆ -0.18181  ┆ … ┆ 0.079083   ┆ 0.1603     ┆ 0.122633   ┆ 0.037433  │\n",
       "└────────────┴───────┴──────────┴───────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | exec: false\n",
    "feature_engineer = FeatureEngineer(\n",
    "    transforms=[\n",
    "        LogReturn(source_field=\"ret\", feature_name=\"log_ret\"),\n",
    "        Variance(source_field=\"price\", feature_name=\"var\"),\n",
    "    ],\n",
    "    n_shifts=3,\n",
    ")\n",
    "df_with_features = feature_engineer.create_features(source_df)\n",
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f19546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def append_from_log_ret(df: pl.DataFrame, new_log_ret: float) -> pl.DataFrame:\n",
    "    \"\"\"Adds a new record to the dataframe based on a log return value.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame containing time series data\n",
    "        new_log_ret: The new log return value to add\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with a new row appended\n",
    "    \"\"\"\n",
    "    # Get the latest date and add one day\n",
    "    last_date = df[\"date\"].max()\n",
    "    new_date = last_date + timedelta(days=1)\n",
    "\n",
    "    # Calculate the new return value from log return\n",
    "    new_ret = np.exp(new_log_ret)\n",
    "\n",
    "    # Get the last price and calculate the new price\n",
    "    last_price = df[\"price\"].tail(1).item()\n",
    "    new_price = last_price * new_ret\n",
    "\n",
    "    # Create a new row\n",
    "    new_row = pl.DataFrame(\n",
    "        {\n",
    "            \"date\": [new_date],\n",
    "            \"price\": [new_price],\n",
    "            \"ret\": [new_ret],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Append the new row to the existing DataFrame\n",
    "    return pl.concat([df, new_row])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
