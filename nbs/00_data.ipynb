{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7fd5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "750a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "import abc\n",
    "from dataclasses import dataclass\n",
    "from datetime import date, timedelta\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import QuantileTransformer as SKQuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9908178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"A class for loading and processing time series data with adjustable parameters.\n",
    "\n",
    "    This class handles loading CSV data, computing returns, and calculating rolling variance.\n",
    "\n",
    "    Attributes:\n",
    "        max_records: Maximum number of records to keep (from the end of the dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_records: int = None):\n",
    "        self.max_records = max_records\n",
    "\n",
    "    def load_data(self, path: str) -> pl.DataFrame:\n",
    "        \"\"\"Load and process time series data from a CSV file.\n",
    "\n",
    "        Args:\n",
    "            path: Path to the CSV file containing the data\n",
    "\n",
    "        Returns:\n",
    "            Processed DataFrame with returns and rolling variance\n",
    "        \"\"\"\n",
    "        df = (\n",
    "            pl.read_csv(path, try_parse_dates=True, infer_schema_length=None)\n",
    "            .rename({\"Date\": \"date\", \"Price\": \"price\"})\n",
    "            .sort(\"date\")\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "            ret=pl.col(\"price\") / pl.col(\"price\").shift(1),\n",
    "        )\n",
    "\n",
    "        if self.max_records is not None:\n",
    "            df = df[-self.max_records :]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcf477dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>price</th><th>ret</th></tr><tr><td>date</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1997-01-07</td><td>3.82</td><td>null</td></tr><tr><td>1997-01-08</td><td>3.8</td><td>0.994764</td></tr><tr><td>1997-01-09</td><td>3.61</td><td>0.95</td></tr><tr><td>1997-01-10</td><td>3.92</td><td>1.085873</td></tr><tr><td>1997-01-13</td><td>4.0</td><td>1.020408</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌────────────┬───────┬──────────┐\n",
       "│ date       ┆ price ┆ ret      │\n",
       "│ ---        ┆ ---   ┆ ---      │\n",
       "│ date       ┆ f64   ┆ f64      │\n",
       "╞════════════╪═══════╪══════════╡\n",
       "│ 1997-01-07 ┆ 3.82  ┆ null     │\n",
       "│ 1997-01-08 ┆ 3.8   ┆ 0.994764 │\n",
       "│ 1997-01-09 ┆ 3.61  ┆ 0.95     │\n",
       "│ 1997-01-10 ┆ 3.92  ┆ 1.085873 │\n",
       "│ 1997-01-13 ┆ 4.0   ┆ 1.020408 │\n",
       "└────────────┴───────┴──────────┘"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | exec: false\n",
    "# Create a data loader with default parameters and load the data\n",
    "data_loader = DataLoader(max_records=9000)\n",
    "source_df = data_loader.load_data(\"./data/ng_daily.csv\")\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2285400a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# | export\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDFFeature\u001b[39;00m(abc\u001b[38;5;241m.\u001b[39mABC):\n\u001b[1;32m      6\u001b[0m     source_field: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      7\u001b[0m     feature_name: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DFFeature(abc.ABC):\n",
    "    source_field: str\n",
    "    feature_name: str\n",
    "    requested_lag: int | None = None\n",
    "    step_size: int = 1\n",
    "\n",
    "    def fit(self, X) -> None:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ZeroBasedMonth(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(\n",
    "            (pl.col(self.source_field).dt.month() - 1).alias(self.feature_name)\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuantileTransformer(DFFeature):\n",
    "    \"\"\"A stateful quantile transformer using sklearn's QuantileTransformer.\"\"\"\n",
    "\n",
    "    n_quantiles: int = 1000\n",
    "    output_distribution: str = \"uniform\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.scaler = SKQuantileTransformer(\n",
    "            n_quantiles=self.n_quantiles,\n",
    "            output_distribution=self.output_distribution,\n",
    "            copy=False,\n",
    "        )\n",
    "        self._fitted = False\n",
    "\n",
    "    def fit(self, df: pl.DataFrame) -> None:\n",
    "        \"\"\"Fit the QuantileTransformer on the source_field.\"\"\"\n",
    "        values = df[self.source_field].to_numpy().reshape(-1, 1)\n",
    "        self.scaler.fit(values)\n",
    "        self._fitted = True\n",
    "\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Transform the source_field with fitted QuantileTransformer.\"\"\"\n",
    "        if not self._fitted:\n",
    "            raise RuntimeError(\n",
    "                \"QuantileTransformer must be fitted before calling extract.\"\n",
    "            )\n",
    "\n",
    "        values = df[self.source_field].to_numpy().reshape(-1, 1)\n",
    "        transformed = self.scaler.transform(values).flatten()\n",
    "\n",
    "        return df.with_columns(pl.Series(name=self.feature_name, values=transformed))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Variance(DFFeature):\n",
    "    rolling_variance_window: int = 3\n",
    "\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(\n",
    "            pl.col(self.source_field)\n",
    "            .rolling_var(self.rolling_variance_window)\n",
    "            .clip(lower_bound=1e-4)\n",
    "            .alias(self.feature_name)\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Square(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(\n",
    "            (pl.col(self.source_field) ** 2).alias(self.feature_name)\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LogReturn(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(pl.col(self.source_field).log().alias(self.feature_name))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Identity(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Derivative(DFFeature):\n",
    "    def extract(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.with_columns(\n",
    "            pl.col(self.source_field).diff(n=self.step_size).alias(self.feature_name)\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureEngineer:\n",
    "    \"\"\"A class for creating lagged features from time series data.\n",
    "\n",
    "    This class handles the creation of lagged (shifted) features that can be used for\n",
    "    GARCH-like models and other time series forecasting tasks.\n",
    "\n",
    "    Attributes:\n",
    "        columns: List of column names to create lags for\n",
    "        n_shifts: Number of lag periods to create\n",
    "        drop_nulls: whether to drop the nulls after rolling window calculations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transforms: list[DFFeature],\n",
    "        n_shifts=3,\n",
    "        drop_nulls: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize the FeatureEngineer.\n",
    "\n",
    "        Args:\n",
    "            columns: List of column names to create lags for (default: ['ret', 'var'])\n",
    "            n_shifts: Number of lag periods to create (default: 3)\n",
    "            drop_nulls: whether to drop the nulls after rolling window calculations\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "        self.columns = [t.feature_name for t in transforms]\n",
    "        self.dtrans = {t.feature_name: t for t in transforms}\n",
    "        self.n_shifts = n_shifts\n",
    "        self.drop_nulls = drop_nulls\n",
    "        self.fitted = False\n",
    "\n",
    "    def create_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"Create lagged features from the input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame containing time series data\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with original columns plus lagged features\n",
    "        \"\"\"\n",
    "        # Create a copy of the dataframe to avoid modifying the original\n",
    "        for t in self.transforms:\n",
    "            if not self.fitted:\n",
    "                t.fit(df)\n",
    "            df = t.extract(df)\n",
    "\n",
    "        if not self.fitted:\n",
    "            self.fitted = True\n",
    "        result_df = df.clone()\n",
    "\n",
    "        # Create lagged features for each specified column\n",
    "        for t in self.transforms:\n",
    "            # Check if column exists in dataframe\n",
    "            col = t.feature_name\n",
    "            if col not in df.columns:\n",
    "                print(f\"Warning: Column '{col}' not found in dataframe. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Create each lag\n",
    "            lags = t.requested_lag if t.requested_lag is not None else self.n_shifts\n",
    "            for shift in range(1, lags + 1):\n",
    "                # Create new column name (e.g., prev_ret_1, prev_var_2)\n",
    "                new_col_name = f\"prev_{col}_{shift}\"\n",
    "\n",
    "                # Add the shifted column to the dataframe\n",
    "                result_df = result_df.with_columns(\n",
    "                    pl.col(col).shift(shift * t.step_size).alias(new_col_name)\n",
    "                )\n",
    "        if self.drop_nulls:\n",
    "            result_df = result_df.drop_nulls()\n",
    "        return result_df\n",
    "\n",
    "    def to_numpy_dict(self, df: pl.DataFrame, drop: set[str] | None = None) -> dict:\n",
    "        \"\"\"Convert the dataframe with lagged features to a dictionary of NumPy arrays.\n",
    "\n",
    "        This method extracts the original columns that were used to create lags,\n",
    "        as well as all the generated lag columns, and converts them to NumPy arrays.\n",
    "        The resulting dictionary can be used directly with NumPyro models.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with lagged features created by create_features()\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping column names to NumPy arrays\n",
    "        \"\"\"\n",
    "        if drop is None:\n",
    "            drop = set()\n",
    "        # Create a new dataframe with the features we want to process\n",
    "        features_df = df.clone()\n",
    "\n",
    "        # Dictionary to store the NumPy arrays\n",
    "        numpy_dict = {}\n",
    "\n",
    "        # Add original columns\n",
    "        for col in self.columns:\n",
    "            if col in drop:\n",
    "                continue\n",
    "            if col in features_df.columns:\n",
    "                numpy_dict[col] = features_df[col].to_numpy()\n",
    "\n",
    "        # Add lagged features\n",
    "        for col in self.columns:\n",
    "            if col in drop:\n",
    "                continue\n",
    "            for shift in range(1, self.n_shifts + 1):\n",
    "                lag_col = f\"prev_{col}_{shift}\"\n",
    "                if lag_col in features_df.columns:\n",
    "                    numpy_dict[lag_col] = features_df[lag_col].to_numpy()\n",
    "\n",
    "        return numpy_dict\n",
    "\n",
    "    def get_iterator(self, site: str | None = None):\n",
    "        if site is None:\n",
    "            site = self.columns\n",
    "        else:\n",
    "            site = [site]\n",
    "        for s in site:\n",
    "            this_site_shifts = self.dtrans[s].requested_lag or self.n_shifts\n",
    "            for shift in range(1, this_site_shifts + 1):\n",
    "                yield (s, shift)\n",
    "\n",
    "    def get_shift_pattern(self, site: str, shift: int, prefix: str = \"\"):\n",
    "        return f\"{prefix}prev_{site}_{shift}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38ef1014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>price</th><th>ret</th><th>log_ret</th><th>var</th><th>var_quantile</th><th>prev_log_ret_1</th><th>prev_log_ret_2</th><th>prev_log_ret_3</th></tr><tr><td>date</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1997-01-13</td><td>4.0</td><td>1.020408</td><td>0.020203</td><td>0.042433</td><td>0.860861</td><td>0.082384</td><td>-0.051293</td><td>-0.005249</td></tr><tr><td>1997-01-14</td><td>4.01</td><td>1.0025</td><td>0.002497</td><td>0.002433</td><td>0.316111</td><td>0.020203</td><td>0.082384</td><td>-0.051293</td></tr><tr><td>1997-01-15</td><td>4.34</td><td>1.082294</td><td>0.079083</td><td>0.037433</td><td>0.847429</td><td>0.002497</td><td>0.020203</td><td>0.082384</td></tr><tr><td>1997-01-16</td><td>4.71</td><td>1.085253</td><td>0.081814</td><td>0.122633</td><td>0.944662</td><td>0.079083</td><td>0.002497</td><td>0.020203</td></tr><tr><td>1997-01-17</td><td>3.91</td><td>0.830149</td><td>-0.186151</td><td>0.1603</td><td>0.95773</td><td>0.081814</td><td>0.079083</td><td>0.002497</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌────────────┬───────┬──────────┬───────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ date       ┆ price ┆ ret      ┆ log_ret   ┆ … ┆ var_quanti ┆ prev_log_r ┆ prev_log_r ┆ prev_log_ │\n",
       "│ ---        ┆ ---   ┆ ---      ┆ ---       ┆   ┆ le         ┆ et_1       ┆ et_2       ┆ ret_3     │\n",
       "│ date       ┆ f64   ┆ f64      ┆ f64       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
       "│            ┆       ┆          ┆           ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64       │\n",
       "╞════════════╪═══════╪══════════╪═══════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ 1997-01-13 ┆ 4.0   ┆ 1.020408 ┆ 0.020203  ┆ … ┆ 0.860861   ┆ 0.082384   ┆ -0.051293  ┆ -0.005249 │\n",
       "│ 1997-01-14 ┆ 4.01  ┆ 1.0025   ┆ 0.002497  ┆ … ┆ 0.316111   ┆ 0.020203   ┆ 0.082384   ┆ -0.051293 │\n",
       "│ 1997-01-15 ┆ 4.34  ┆ 1.082294 ┆ 0.079083  ┆ … ┆ 0.847429   ┆ 0.002497   ┆ 0.020203   ┆ 0.082384  │\n",
       "│ 1997-01-16 ┆ 4.71  ┆ 1.085253 ┆ 0.081814  ┆ … ┆ 0.944662   ┆ 0.079083   ┆ 0.002497   ┆ 0.020203  │\n",
       "│ 1997-01-17 ┆ 3.91  ┆ 0.830149 ┆ -0.186151 ┆ … ┆ 0.95773    ┆ 0.081814   ┆ 0.079083   ┆ 0.002497  │\n",
       "└────────────┴───────┴──────────┴───────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | exec: false\n",
    "feature_engineer = FeatureEngineer(\n",
    "    transforms=[\n",
    "        LogReturn(source_field=\"ret\", feature_name=\"log_ret\"),\n",
    "        Variance(source_field=\"price\", feature_name=\"var\", requested_lag=0),\n",
    "        QuantileTransformer(\n",
    "            source_field=\"var\", feature_name=\"var_quantile\", requested_lag=0\n",
    "        ),\n",
    "    ],\n",
    "    n_shifts=3,\n",
    ")\n",
    "df_with_features = feature_engineer.create_features(source_df)\n",
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f203e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>price</th><th>ret</th><th>price_quantile</th></tr><tr><td>date</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1997-01-07</td><td>3.82</td><td>null</td><td>0.573073</td></tr><tr><td>1997-01-08</td><td>3.8</td><td>0.994764</td><td>0.569069</td></tr><tr><td>1997-01-09</td><td>3.61</td><td>0.95</td><td>0.535536</td></tr><tr><td>1997-01-10</td><td>3.92</td><td>1.085873</td><td>0.591091</td></tr><tr><td>1997-01-13</td><td>4.0</td><td>1.020408</td><td>0.607107</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌────────────┬───────┬──────────┬────────────────┐\n",
       "│ date       ┆ price ┆ ret      ┆ price_quantile │\n",
       "│ ---        ┆ ---   ┆ ---      ┆ ---            │\n",
       "│ date       ┆ f64   ┆ f64      ┆ f64            │\n",
       "╞════════════╪═══════╪══════════╪════════════════╡\n",
       "│ 1997-01-07 ┆ 3.82  ┆ null     ┆ 0.573073       │\n",
       "│ 1997-01-08 ┆ 3.8   ┆ 0.994764 ┆ 0.569069       │\n",
       "│ 1997-01-09 ┆ 3.61  ┆ 0.95     ┆ 0.535536       │\n",
       "│ 1997-01-10 ┆ 3.92  ┆ 1.085873 ┆ 0.591091       │\n",
       "│ 1997-01-13 ┆ 4.0   ┆ 1.020408 ┆ 0.607107       │\n",
       "└────────────┴───────┴──────────┴────────────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | exec: false\n",
    "qt = QuantileTransformer(source_field=\"price\", feature_name=\"price_quantile\")\n",
    "qt.fit(source_df)\n",
    "source_df = qt.extract(source_df)\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f19546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def append_from_log_ret(\n",
    "    df: pl.DataFrame,\n",
    "    new_log_ret: float,\n",
    "    inherit_vals: list[str],\n",
    "    add_variables: dict[str, float],\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Adds a new record to the dataframe based on a log return value.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame containing time series data\n",
    "        new_log_ret: The new log return value to add\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with a new row appended\n",
    "    \"\"\"\n",
    "    # Get the latest date and add one day\n",
    "    last_date = df[\"date\"].max()\n",
    "    new_date = last_date + timedelta(days=1)\n",
    "\n",
    "    # Calculate the new return value from log return\n",
    "    new_ret = np.exp(new_log_ret)\n",
    "\n",
    "    # Get the last price and calculate the new price\n",
    "    last_price = df[\"price\"].tail(1).item()\n",
    "    new_price = last_price * new_ret\n",
    "    d = {\n",
    "        \"date\": [new_date],\n",
    "        \"price\": [new_price],\n",
    "        \"ret\": [new_ret],\n",
    "    }\n",
    "    for c, v in add_variables.items():\n",
    "        d[c] = [v]\n",
    "\n",
    "    for c in inherit_vals:\n",
    "        d[c] = [df[c].tail(1).item()]\n",
    "    # Create a new row\n",
    "    new_row = pl.DataFrame(d)\n",
    "\n",
    "    # Append the new row to the existing DataFrame\n",
    "    return pl.concat([df, new_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e749e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def binary_feature_from_date_ranges(\n",
    "    date_range: tuple[date, date],\n",
    "    periods: list[tuple[date, date]],\n",
    "    feature_name: str = \"feature\",\n",
    ") -> pl.DataFrame:\n",
    "    # Create a date range dataframe\n",
    "    df = pl.DataFrame(\n",
    "        {\n",
    "            \"date\": pl.date_range(\n",
    "                start=date_range[0], end=date_range[1], interval=\"1d\", eager=True\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Start with all zeros\n",
    "    df = df.with_columns(pl.lit(0).alias(feature_name))\n",
    "\n",
    "    # For each period, set the feature to 1 if the date is within the range\n",
    "    for start, end in periods:\n",
    "        df = df.with_columns(\n",
    "            pl.when((pl.col(\"date\") >= start) & (pl.col(\"date\") <= end))\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(feature_name))\n",
    "            .alias(feature_name)\n",
    "            .cast(pl.Int64)\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c49e60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_845, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>RU/UA_war</th></tr><tr><td>date</td><td>i64</td></tr></thead><tbody><tr><td>2010-01-01</td><td>0</td></tr><tr><td>2010-01-02</td><td>0</td></tr><tr><td>2010-01-03</td><td>0</td></tr><tr><td>2010-01-04</td><td>0</td></tr><tr><td>2010-01-05</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2025-12-28</td><td>1</td></tr><tr><td>2025-12-29</td><td>1</td></tr><tr><td>2025-12-30</td><td>1</td></tr><tr><td>2025-12-31</td><td>1</td></tr><tr><td>2026-01-01</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_845, 2)\n",
       "┌────────────┬───────────┐\n",
       "│ date       ┆ RU/UA_war │\n",
       "│ ---        ┆ ---       │\n",
       "│ date       ┆ i64       │\n",
       "╞════════════╪═══════════╡\n",
       "│ 2010-01-01 ┆ 0         │\n",
       "│ 2010-01-02 ┆ 0         │\n",
       "│ 2010-01-03 ┆ 0         │\n",
       "│ 2010-01-04 ┆ 0         │\n",
       "│ 2010-01-05 ┆ 0         │\n",
       "│ …          ┆ …         │\n",
       "│ 2025-12-28 ┆ 1         │\n",
       "│ 2025-12-29 ┆ 1         │\n",
       "│ 2025-12-30 ┆ 1         │\n",
       "│ 2025-12-31 ┆ 1         │\n",
       "│ 2026-01-01 ┆ 1         │\n",
       "└────────────┴───────────┘"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "binary_feature_from_date_ranges(\n",
    "    date_range=(date(2010, 1, 1), date(2026, 1, 1)),\n",
    "    periods=[\n",
    "        (date(2022, 2, 24), date(2026, 1, 1)),\n",
    "    ],\n",
    "    feature_name=\"RU/UA_war\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
